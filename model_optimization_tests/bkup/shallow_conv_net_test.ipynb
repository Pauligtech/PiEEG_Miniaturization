{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.pick_types is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.pick_channels_regexp is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.channel_type is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "/home/arazzz/anaconda3/envs/moabb_model_optimization/lib/python3.11/site-packages/moabb/pipelines/__init__.py:26: ModuleNotFoundError: Tensorflow is not installed. You won't be able to use these MOABB pipelines if you attempt to do so.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import glob\n",
    "import pickle\n",
    "import tqdm\n",
    "import copy\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mne\n",
    "from rich import print as rprint\n",
    "from rich.pretty import pprint as rpprint\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "# JAX + Keras\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"0\"\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Permute, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.layers import SeparableConv2D, DepthwiseConv2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import SpatialDropout2D\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.layers import Input, Flatten\n",
    "from keras.constraints import max_norm\n",
    "from keras import backend as K\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, TimeDistributed, BatchNormalization\n",
    "import h5py\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Dataset\n",
    "from custom_datasets.fatigue_mi import FatigueMI\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-01 23:10:59.891010: W external/xla/xla/service/gpu/nvptx_compiler.cc:744] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.107). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "SKLRNG = 42\n",
    "RNG = jax.random.PRNGKey(SKLRNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding metadata with 3 columns\n"
     ]
    }
   ],
   "source": [
    "def data_generator(dataset, subjects = [1], channel_idx = [], filters = ([8, 32],), sfreq = 250):\n",
    "\n",
    "    find_events = lambda raw, event_id: mne.find_events(raw, shortest_event=0, verbose=False) if len(mne.utils._get_stim_channel(None, raw.info, raise_error=False)) > 0 else mne.events_from_annotations(raw, event_id=event_id, verbose=False)[0]\n",
    "    \n",
    "    data = dataset.get_data(subjects=subjects)\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    metadata = []\n",
    "\n",
    "    for subject_id in data.keys():\n",
    "        for session_id in data[subject_id].keys():\n",
    "            for run_id in data[subject_id][session_id].keys():\n",
    "                raw = data[subject_id][session_id][run_id]\n",
    "                \n",
    "                for fmin, fmax in filters:\n",
    "                    raw = raw.filter(l_freq = fmin, h_freq = fmax, method = 'iir', picks = 'eeg', verbose = False)\n",
    "                \n",
    "                events = find_events(raw, dataset.event_id)\n",
    "\n",
    "                tmin = dataset.interval[0]\n",
    "                tmax = dataset.interval[1]\n",
    "\n",
    "                channels = np.asarray(raw.info['ch_names'])[channel_idx] if len(channel_idx) > 0 else np.asarray(raw.info['ch_names'])\n",
    "\n",
    "                # rpprint(channels)\n",
    "                \n",
    "                stim_channels = mne.utils._get_stim_channel(None, raw.info, raise_error=False)\n",
    "                picks = mne.pick_channels(raw.info[\"ch_names\"], include=channels, exclude=stim_channels, ordered=True)\n",
    "\n",
    "                x = mne.Epochs(\n",
    "                    raw,\n",
    "                    events,\n",
    "                    event_id=dataset.event_id,\n",
    "                    tmin=tmin,\n",
    "                    tmax=tmax,\n",
    "                    proj=False,\n",
    "                    baseline=None,\n",
    "                    preload=True,\n",
    "                    verbose=False,\n",
    "                    picks=picks,\n",
    "                    event_repeated=\"drop\",\n",
    "                    on_missing=\"ignore\",\n",
    "                )\n",
    "                x_events = x.events\n",
    "                inv_events = {k: v for v, k in dataset.event_id.items()}\n",
    "                labels = [inv_events[e] for e in x_events[:, -1]]\n",
    "\n",
    "                # rpprint({\n",
    "                #     \"X\": np.asarray(x.get_data(copy=False)).shape,\n",
    "                #     \"y\": np.asarray(labels).shape,\n",
    "                #     \"channels selected\": np.asarray(raw.info['ch_names'])[channel_idx]\n",
    "                # })\n",
    "\n",
    "                # x.plot(scalings=\"auto\")\n",
    "                # display(x.info)\n",
    "                \n",
    "                x_resampled = x.resample(sfreq) # Resampler_Epoch\n",
    "                x_resampled_data = x_resampled.get_data(copy=False) # Convert_Epoch_Array\n",
    "                x_resampled_data_standard_scaler = np.asarray([\n",
    "                    StandardScaler().fit_transform(x_resampled_data[i])\n",
    "                    for i in np.arange(x_resampled_data.shape[0])\n",
    "                ]) # Standard_Scaler_Epoch\n",
    "\n",
    "                # x_resampled.plot(scalings=\"auto\")\n",
    "                # display(x_resampled.info)\n",
    "\n",
    "                n = x_resampled_data_standard_scaler.shape[0]\n",
    "                # n = x.get_data(copy=False).shape[0]\n",
    "                met = pd.DataFrame(index=range(n))\n",
    "                met[\"subject\"] = subject_id\n",
    "                met[\"session\"] = session_id\n",
    "                met[\"run\"] = run_id\n",
    "                x.metadata = met.copy()\n",
    "                \n",
    "                # X.append(x_resampled_data_standard_scaler)\n",
    "                X.append(x)\n",
    "                y.append(labels)\n",
    "                metadata.append(met)\n",
    "\n",
    "    return np.concatenate(X, axis=0), np.concatenate(y), pd.concat(metadata, ignore_index=True)\n",
    "\n",
    "fat_dataset = FatigueMI()\n",
    "# X, y, _ = data_generator(fat_dataset, subjects=fat_dataset.subject_list, sfreq=250)\n",
    "X, y, _ = data_generator(fat_dataset, subjects=[5], channel_idx=[], sfreq=128)\n",
    "# X, y, _ = data_generator(fat_dataset, subjects=[6], channel_idx=[1, 2, 5, 12, 13, 14, 17, 18], sfreq=128)\n",
    "# X, y, _ = data_generator(fat_dataset, subjects=[5, 6, 12, 17, 24, 28, 29], channel_idx=[], sfreq=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'X'</span>: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">108</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'y'</span>: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">108</span>,<span style=\"font-weight: bold\">)}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'X'\u001b[0m: \u001b[1m(\u001b[0m\u001b[1;36m108\u001b[0m, \u001b[1;36m20\u001b[0m, \u001b[1;36m256\u001b[0m\u001b[1m)\u001b[0m, \u001b[32m'y'\u001b[0m: \u001b[1m(\u001b[0m\u001b[1;36m108\u001b[0m,\u001b[1m)\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'NUM_SAMPLES'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'NUM_CHANNELS'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'NUM_CLASSES'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'NUM_SAMPLES'\u001b[0m: \u001b[1;36m256\u001b[0m, \u001b[32m'NUM_CHANNELS'\u001b[0m: \u001b[1;36m20\u001b[0m, \u001b[32m'NUM_CLASSES'\u001b[0m: \u001b[1;36m2\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_encoded = LabelEncoder().fit_transform(y) # 0 = left, 1 = right, 2 = unlabelled (!NOTE: should be ignored for binary classification)\n",
    "# rpprint(y_encoded)\n",
    "rprint({\n",
    "    \"X\": X.shape,\n",
    "    \"y\": y_encoded.shape\n",
    "})\n",
    "\n",
    "NUM_SAMPLES = X.shape[-1]\n",
    "NUM_CHANNELS = X.shape[-2]\n",
    "NUM_CLASSES = len(np.unique(y_encoded))\n",
    "\n",
    "rpprint({\n",
    "    \"NUM_SAMPLES\": NUM_SAMPLES,\n",
    "    \"NUM_CHANNELS\": NUM_CHANNELS,\n",
    "    \"NUM_CLASSES\": NUM_CLASSES\n",
    "})\n",
    "\n",
    "# sns.histplot(y_encoded); # Plot the class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'X_train'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'86 (79.63%)'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'X_test'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'22 (20.37%)'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'X_train'\u001b[0m: \u001b[32m'86 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m79.63%\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'X_test'\u001b[0m: \u001b[32m'22 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m20.37%\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TRAIN_SIZE = 0.8\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X.reshape(-1, NUM_SAMPLES, NUM_CHANNELS), y_encoded, test_size=TEST_SIZE, random_state=SKLRNG, stratify=None, shuffle=False)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X.reshape(-1, NUM_SAMPLES, NUM_CHANNELS), y_encoded, test_size=TEST_SIZE, random_state=SKLRNG, stratify=y_encoded, shuffle=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=TEST_SIZE, random_state=SKLRNG, stratify=y_encoded, shuffle=True)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=TEST_SIZE, random_state=SKLRNG, stratify=None, shuffle=False)\n",
    "\n",
    "total = X_train.shape[0] + X_test.shape[0]\n",
    "rpprint({\n",
    "    \"X_train\": f\"{X_train.shape[0]} ({X_train.shape[0] / total * 100:.2f}%)\",\n",
    "    \"X_test\": f\"{X_test.shape[0]} ({X_test.shape[0] / total * 100:.2f}%)\",\n",
    "}, expand_all=True)\n",
    "\n",
    "# batch_size = 32\n",
    "# batches = batch_generator(X_train, batch_size, RNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86, 20, 256)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region Helper funcs\n",
    "def shallow_conv_net_square_layer(x):\n",
    "    return jnp.square(x)\n",
    "\n",
    "def shallow_conv_net_log_layer(x):\n",
    "    return jnp.log(jnp.clip(x, 1e-7, 10000))\n",
    "\n",
    "CUSTOM_OBJECTS = {\n",
    "    \"shallow_conv_net_square_layer\": shallow_conv_net_square_layer, \n",
    "    \"shallow_conv_net_log_layer\": shallow_conv_net_log_layer \n",
    "}\n",
    "# endregion Helper funcs\n",
    "\n",
    "# region Models\n",
    "def eeg_net(nb_classes = NUM_CLASSES, channels = NUM_CHANNELS, samples = NUM_SAMPLES, dropout_rate = 0.5, \n",
    "            kernLength = 64, F1 = 8, D = 2, F2 = 16, norm_rate = 0.25, dropoutType = 'Dropout'):\n",
    "    \"\"\"\n",
    "        From: https://github.com/vlawhern/arl-eegmodels/blob/master/EEGModels.py\n",
    "    \"\"\"\n",
    "\n",
    "    dropoutType = {'Dropout': Dropout, 'SpatialDropout2D': SpatialDropout2D}[dropoutType]\n",
    "\n",
    "    input1   = Input(shape = (channels, samples, 1))\n",
    "\n",
    "    ##################################################################\n",
    "    block1       = Conv2D(F1, (1, kernLength), padding = 'same',\n",
    "                                   input_shape = (channels, samples, 1),\n",
    "                                   use_bias = False)(input1)\n",
    "    block1       = BatchNormalization()(block1)\n",
    "    block1       = DepthwiseConv2D((channels, 1), use_bias = False, \n",
    "                                   depth_multiplier = D,\n",
    "                                   depthwise_constraint = max_norm(1.))(block1)\n",
    "    block1       = BatchNormalization()(block1)\n",
    "    block1       = Activation('elu')(block1)\n",
    "    block1       = AveragePooling2D((1, 4))(block1)\n",
    "    block1       = dropoutType(dropout_rate)(block1)\n",
    "    \n",
    "    block2       = SeparableConv2D(F2, (1, 16), use_bias = False, padding = 'same')(block1)\n",
    "    block2       = BatchNormalization()(block2)\n",
    "    block2       = Activation('elu')(block2)\n",
    "    block2       = AveragePooling2D((1, 8))(block2)\n",
    "    block2       = dropoutType(dropout_rate)(block2)\n",
    "        \n",
    "    flatten      = Flatten(name = 'flatten')(block2)\n",
    "    \n",
    "    dense        = Dense(nb_classes, name = 'dense', \n",
    "                         kernel_constraint = max_norm(norm_rate))(flatten)\n",
    "    softmax      = Activation('softmax', name = 'softmax')(dense)\n",
    "    \n",
    "    return Model(inputs=input1, outputs=softmax)\n",
    "\n",
    "def deep_conv_net(nb_classes = NUM_CLASSES, channels = NUM_CHANNELS, samples = NUM_SAMPLES, dropout_rate = 0.5):\n",
    "    \"\"\"\n",
    "        From: https://github.com/vlawhern/arl-eegmodels/blob/master/EEGModels.py\n",
    "    \"\"\"\n",
    "    input_main   = Input((channels, samples, 1))\n",
    "    block1       = Conv2D(25, (1, 5), \n",
    "                                 input_shape=(channels, samples, 1),\n",
    "                                 kernel_constraint = max_norm(2., axis=(0,1,2)))(input_main)\n",
    "    block1       = Conv2D(25, (channels, 1),\n",
    "                                 kernel_constraint = max_norm(2., axis=(0,1,2)))(block1)\n",
    "    block1       = BatchNormalization(epsilon=1e-05, momentum=0.9)(block1)\n",
    "    block1       = Activation('elu')(block1)\n",
    "    block1       = MaxPooling2D(pool_size=(1, 2), strides=(1, 2))(block1)\n",
    "    block1       = Dropout(dropout_rate)(block1)\n",
    "  \n",
    "    block2       = Conv2D(50, (1, 5),\n",
    "                                 kernel_constraint = max_norm(2., axis=(0,1,2)))(block1)\n",
    "    block2       = BatchNormalization(epsilon=1e-05, momentum=0.9)(block2)\n",
    "    block2       = Activation('elu')(block2)\n",
    "    block2       = MaxPooling2D(pool_size=(1, 2), strides=(1, 2))(block2)\n",
    "    block2       = Dropout(dropout_rate)(block2)\n",
    "    \n",
    "    block3       = Conv2D(100, (1, 5),\n",
    "                                 kernel_constraint = max_norm(2., axis=(0,1,2)))(block2)\n",
    "    block3       = BatchNormalization(epsilon=1e-05, momentum=0.9)(block3)\n",
    "    block3       = Activation('elu')(block3)\n",
    "    block3       = MaxPooling2D(pool_size=(1, 2), strides=(1, 2))(block3)\n",
    "    block3       = Dropout(dropout_rate)(block3)\n",
    "    \n",
    "    block4       = Conv2D(200, (1, 5),\n",
    "                                 kernel_constraint = max_norm(2., axis=(0,1,2)))(block3)\n",
    "    block4       = BatchNormalization(epsilon=1e-05, momentum=0.9)(block4)\n",
    "    block4       = Activation('elu')(block4)\n",
    "    block4       = MaxPooling2D(pool_size=(1, 2), strides=(1, 2))(block4)\n",
    "    block4       = Dropout(dropout_rate)(block4)\n",
    "    \n",
    "    flatten      = Flatten()(block4)\n",
    "    \n",
    "    dense        = Dense(nb_classes, kernel_constraint = max_norm(0.5))(flatten)\n",
    "    softmax      = Activation('softmax')(dense)\n",
    "    \n",
    "    return Model(inputs=input_main, outputs=softmax)\n",
    "\n",
    "def shallow_conv_net(nb_classes = NUM_CLASSES, channels = NUM_CHANNELS, samples = NUM_SAMPLES, **kwargs):\n",
    "    \"\"\"\n",
    "        From: https://github.com/vlawhern/arl-eegmodels/blob/master/EEGModels.py\n",
    "    \"\"\"\n",
    "    \n",
    "    _POOL_SIZE_D2_ = kwargs.get(\"pool_size_d2\", 35)\n",
    "    _STRIDES_D2_ = kwargs.get(\"strides_d2\", 7)\n",
    "    _CONV_FILTERS_D2_ = kwargs.get(\"conv_filters_d2\", 13)\n",
    "\n",
    "    _POOL_SIZE_ = kwargs.get(\"pool_size\", (1, _POOL_SIZE_D2_))\n",
    "    _STRIDES_ = kwargs.get(\"strides\", (1, _STRIDES_D2_))\n",
    "    _CONV_FILTERS_ = kwargs.get(\"conv_filters\", (1, _CONV_FILTERS_D2_))\n",
    "\n",
    "    _CONV2D_1_UNITS_ = kwargs.get(\"conv2d_1_units\", 40)\n",
    "    _CONV2D_2_UNITS_ = kwargs.get(\"conv2d_2_units\", 40)\n",
    "    _L2_REG_1_ = kwargs.get(\"l2_reg_1\", 0.01)\n",
    "    _L2_REG_2_ = kwargs.get(\"l2_reg_2\", 0.01)\n",
    "    _L2_REG_3_ = kwargs.get(\"l2_reg_3\", 0.01)\n",
    "    _DROPOUT_RATE_ = kwargs.get(\"dropout_rate\", 0.5)\n",
    "\n",
    "    input_main   = Input(shape=(channels, samples, 1))\n",
    "    block1       = Conv2D(_CONV2D_1_UNITS_, _CONV_FILTERS_,\n",
    "                                 input_shape=(channels, samples, 1),\n",
    "                                 kernel_constraint = max_norm(2., axis=(0,1,2)), kernel_regularizer=keras.regularizers.L2(_L2_REG_1_))(input_main)\n",
    "    # block1       = Conv2D(40, (channels, 1), use_bias=False, \n",
    "    #                       kernel_constraint = max_norm(2., axis=(0,1,2)))(block1)\n",
    "    block1       = Conv2D(_CONV2D_2_UNITS_, (channels, 1), use_bias=False, \n",
    "                          kernel_constraint = max_norm(2., axis=(0,1,2)), kernel_regularizer=keras.regularizers.L2(_L2_REG_2_))(block1)\n",
    "    block1       = BatchNormalization(epsilon=1e-05, momentum=0.9)(block1)\n",
    "    block1       = Activation(shallow_conv_net_square_layer)(block1)\n",
    "    block1       = AveragePooling2D(pool_size=_POOL_SIZE_, strides=_STRIDES_)(block1)\n",
    "    block1       = Activation(shallow_conv_net_log_layer)(block1)\n",
    "    block1       = Dropout(_DROPOUT_RATE_)(block1)\n",
    "    flatten      = Flatten()(block1)\n",
    "    # dense        = Dense(nb_classes, kernel_constraint = max_norm(0.5))(flatten)\n",
    "    dense        = Dense(nb_classes, kernel_constraint = max_norm(0.5), kernel_regularizer=keras.regularizers.L2(_L2_REG_3_))(flatten)\n",
    "    softmax      = Activation('softmax')(dense)\n",
    "    \n",
    "    return Model(inputs=input_main, outputs=softmax)\n",
    "\n",
    "def lstm_net(nb_classes = NUM_CLASSES, channels = NUM_CHANNELS, samples = NUM_SAMPLES):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(samples, channels)))\n",
    "    model.add(LSTM(40, return_sequences=True, stateful=False))\n",
    "    # model.add(LSTM(40, return_sequences=True, stateful=False, batch_input_shape=(batch_size, timesteps, data_dim)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(40, return_sequences=True, stateful=False))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(40, stateful=False))\n",
    "    model.add(Dropout(0.5))\n",
    "    # model.add(TimeDistributed(Dense(T_train)))\n",
    "    model.add(Dense(50,activation='softmax'))\n",
    "    model.add(Dense(nb_classes, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def lstm_cnn_net(nb_classes = NUM_CLASSES, channels = NUM_CHANNELS, samples = NUM_SAMPLES, **kwargs):\n",
    "\n",
    "    _CONV1D_1_UNITS_ = kwargs.get(\"conv1d_1_units\", 40)\n",
    "    _CONV1D_1_KERNEL_SIZE_ = kwargs.get(\"conv1d_1_kernel_size\", 20)\n",
    "    _CONV1D_1_STRIDES_ = kwargs.get(\"conv1d_1_strides\", 4)\n",
    "    _CONV1D_1_MAXPOOL_SIZE_ = kwargs.get(\"conv1d_1_maxpool_size\", 4)\n",
    "    _CONV1D_1_MAXPOOL_STRIDES_ = kwargs.get(\"conv1d_1_maxpool_strides\", 4)\n",
    "    _LSTM_1_UNITS_ = kwargs.get(\"lstm_1_units\", 50)\n",
    "    _L2_REG_1_ = kwargs.get(\"l2_reg_1\", 0.01)\n",
    "    _L2_REG_2_ = kwargs.get(\"l2_reg_2\", 0.01)\n",
    "    _L2_REG_3_ = kwargs.get(\"l2_reg_3\", 0.01)\n",
    "    _DROPOUT_RATE_1_ = kwargs.get(\"dropout_rate_1\", 0.5)\n",
    "    _DROPOUT_RATE_2_ = kwargs.get(\"dropout_rate_2\", 0.5)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(samples, channels)))\n",
    "\n",
    "    # add 1-layer cnn\n",
    "    model.add(Conv1D(_CONV1D_1_UNITS_, kernel_size=_CONV1D_1_KERNEL_SIZE_, strides=_CONV1D_1_STRIDES_, kernel_regularizer=keras.regularizers.L2(_L2_REG_1_)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(_DROPOUT_RATE_1_))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=_CONV1D_1_MAXPOOL_SIZE_, strides=_CONV1D_1_MAXPOOL_STRIDES_))\n",
    "\n",
    "\n",
    "    # add 1-layer lstm\n",
    "    model.add(LSTM(_LSTM_1_UNITS_, return_sequences=True, stateful=False, kernel_regularizer=keras.regularizers.L2(_L2_REG_2_)))\n",
    "    model.add(Dropout(_DROPOUT_RATE_2_))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(nb_classes, activation='softmax', kernel_regularizer=keras.regularizers.L2(_L2_REG_3_)))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def lstm_cnn_net_v2(nb_classes = NUM_CLASSES, channels = NUM_CHANNELS, samples = NUM_SAMPLES):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(samples, channels)))\n",
    "\n",
    "    # Add 1-layer LSTM\n",
    "    model.add(LSTM(50, return_sequences=True, stateful=False))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    # Add 1-layer CNN\n",
    "    model.add(Conv1D(40, kernel_size=20, strides=4))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=4, strides=4))\n",
    "    model.add(Flatten())\n",
    "    # Fully connected layer for classification\n",
    "    model.add(Dense(nb_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "# endregion Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLDS = 5\n",
    "kfold = KFold(n_splits=NUM_FOLDS, shuffle=True)\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "model = shallow_conv_net()\n",
    "# model = lstm_cnn_net()\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for train, test in kfold.split(X_train, y_train):\n",
    "        # Fit data to model\n",
    "        model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "        \n",
    "        history = model.fit(\n",
    "                X_train[train],\n",
    "                y_train[train],\n",
    "                batch_size=64,\n",
    "                epochs=25,\n",
    "                validation_data=(X_train[test], y_encoded[test]),\n",
    "                # validation_split=0.2,\n",
    "                shuffle=True,\n",
    "                callbacks=[\n",
    "                        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=75, restore_best_weights=True),\n",
    "                        keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=75, factor=0.5)\n",
    "                        # keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True),\n",
    "                        # keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=3, factor=0.1)\n",
    "                ],\n",
    "        )\n",
    "        \n",
    "        # Generate generalization metrics\n",
    "        scores = model.evaluate(X_train[test], y_train[test], verbose=0)\n",
    "        rprint(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "        acc_per_fold.append(scores[1] * 100)\n",
    "        loss_per_fold.append(scores[0])\n",
    "        \n",
    "        # Increase fold number\n",
    "        fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cross_val_score = np.mean(acc_per_fold)\n",
    "rprint(\"Mean Cross Validation Score (5-fold):\", mean_cross_val_score)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)\n",
    "rprint(\"Test Accuracy:\", results_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_val = model.evaluate(X_val, y_val)\n",
    "results_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "# X_test.reshape(-1, NUM_SAMPLES, NUM_CHANNELS).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = shallow_conv_net()\n",
    "# model = lstm_cnn_net()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=128,\n",
    "    epochs=25,\n",
    "    # validation_data=(X_test, y_test),\n",
    "    validation_split=0.2,\n",
    "    shuffle=True,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=75, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=75, factor=0.5)\n",
    "        # keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True),\n",
    "        # keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=3, factor=0.1)\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_256375/3469357869.py:151: ExperimentalWarning: NSGAIIISampler is experimental (supported from v3.2.0). The interface can change in the future.\n",
      "  study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.NSGAIIISampler())\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3219bab47c30458791461000d2cff381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding metadata with 3 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arazzz/anaconda3/envs/moabb_model_optimization/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 887ms/step - accuracy: 0.4805 - loss: 17.3617 - val_accuracy: 0.5000 - val_loss: 12.5354 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6668 - loss: 12.1372 - val_accuracy: 0.3333 - val_loss: 11.1348 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6399 - loss: 10.8530 - val_accuracy: 0.4444 - val_loss: 10.0629 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7500 - loss: 9.7167 - val_accuracy: 0.5000 - val_loss: 9.1637 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7137 - loss: 8.9267 - val_accuracy: 0.5556 - val_loss: 8.4057 - learning_rate: 0.0010\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 432ms/step - accuracy: 0.4545 - loss: 8.3827\n",
      "\n",
      "\n",
      "Adding metadata with 3 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arazzz/anaconda3/envs/moabb_model_optimization/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.5294 - loss: 67.4647 - val_accuracy: 0.5556 - val_loss: 56.4078 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6618 - loss: 56.3769 - val_accuracy: 0.5556 - val_loss: 50.2950 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6471 - loss: 49.6135 - val_accuracy: 0.5556 - val_loss: 46.5866 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5588 - loss: 45.3197 - val_accuracy: 0.5556 - val_loss: 42.1449 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7206 - loss: 40.6352 - val_accuracy: 0.6111 - val_loss: 37.4952 - learning_rate: 0.0010\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 737ms/step - accuracy: 0.5455 - loss: 37.5470\n",
      "\n",
      "\n",
      "Sampling frequency of the instance is already 300.0, returning unmodified.\n",
      "Adding metadata with 3 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arazzz/anaconda3/envs/moabb_model_optimization/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.4706 - loss: 37.2294 - val_accuracy: 0.5556 - val_loss: 30.3164 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.5735 - loss: 30.3119 - val_accuracy: 0.5000 - val_loss: 30.6231 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.6029 - loss: 28.7373 - val_accuracy: 0.6667 - val_loss: 24.5432 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9412 - loss: 24.1468 - val_accuracy: 0.5000 - val_loss: 22.9223 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9559 - loss: 22.0214 - val_accuracy: 0.5556 - val_loss: 20.8703 - learning_rate: 0.0010\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489ms/step - accuracy: 0.5000 - loss: 21.1185\n",
      "\n",
      "\n",
      "Adding metadata with 3 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arazzz/anaconda3/envs/moabb_model_optimization/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.5294 - loss: 36.2586 - val_accuracy: 0.5556 - val_loss: 30.5167 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6324 - loss: 30.5104 - val_accuracy: 0.5000 - val_loss: 27.7761 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.6176 - loss: 27.3057 - val_accuracy: 0.4444 - val_loss: 25.2891 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7206 - loss: 24.8212 - val_accuracy: 0.5000 - val_loss: 23.2991 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9412 - loss: 22.6900 - val_accuracy: 0.4444 - val_loss: 21.4162 - learning_rate: 0.0010\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 490ms/step - accuracy: 0.4545 - loss: 21.4374\n",
      "\n",
      "\n",
      "Adding metadata with 3 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arazzz/anaconda3/envs/moabb_model_optimization/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 854ms/step - accuracy: 0.5455 - loss: 44.6405 - val_accuracy: 0.4444 - val_loss: 35.5819 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5905 - loss: 34.4208 - val_accuracy: 0.5556 - val_loss: 30.7721 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6668 - loss: 29.9487 - val_accuracy: 0.6667 - val_loss: 27.1901 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6776 - loss: 26.5007 - val_accuracy: 0.6667 - val_loss: 24.2216 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8263 - loss: 23.6273 - val_accuracy: 0.5556 - val_loss: 21.6819 - learning_rate: 0.0010\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 427ms/step - accuracy: 0.5909 - loss: 21.6849\n",
      "\n",
      "\n",
      "Adding metadata with 3 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arazzz/anaconda3/envs/moabb_model_optimization/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.4770 - loss: 70.8420 - val_accuracy: 0.5556 - val_loss: 52.6310 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.4926 - loss: 48.6858 - val_accuracy: 0.5000 - val_loss: 41.0236 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5949 - loss: 37.5694 - val_accuracy: 0.5556 - val_loss: 31.7768 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6507 - loss: 30.2368 - val_accuracy: 0.5556 - val_loss: 26.3650 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8610 - loss: 24.4748 - val_accuracy: 0.5556 - val_loss: 21.4303 - learning_rate: 0.0010\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 667ms/step - accuracy: 0.4545 - loss: 21.5249\n",
      "\n",
      "\n",
      "Adding metadata with 3 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arazzz/anaconda3/envs/moabb_model_optimization/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.5441 - loss: 223.2331 - val_accuracy: 0.6667 - val_loss: 167.5450 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8088 - loss: 167.4390 - val_accuracy: 0.5000 - val_loss: 137.0431 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7794 - loss: 136.7630 - val_accuracy: 0.6111 - val_loss: 115.4525 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.7941 - loss: 115.3132 - val_accuracy: 0.4444 - val_loss: 100.4441 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.7059 - loss: 98.8379 - val_accuracy: 0.5556 - val_loss: 87.1152 - learning_rate: 0.0010\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 615ms/step - accuracy: 0.5000 - loss: 87.4675\n",
      "\n",
      "\n",
      "Sampling frequency of the instance is already 300.0, returning unmodified.\n",
      "Adding metadata with 3 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arazzz/anaconda3/envs/moabb_model_optimization/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.6471 - loss: 23.2521 - val_accuracy: 0.3889 - val_loss: 20.1847 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8529 - loss: 19.8326 - val_accuracy: 0.3889 - val_loss: 17.3881 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9853 - loss: 16.8836 - val_accuracy: 0.4444 - val_loss: 15.4585 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9559 - loss: 14.8113 - val_accuracy: 0.5556 - val_loss: 14.6348 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8676 - loss: 13.3040 - val_accuracy: 0.5000 - val_loss: 15.0149 - learning_rate: 0.0010\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 769ms/step - accuracy: 0.5000 - loss: 14.8747\n",
      "\n",
      "\n",
      "Adding metadata with 3 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arazzz/anaconda3/envs/moabb_model_optimization/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.4706 - loss: 73.3815 - val_accuracy: 0.4444 - val_loss: 58.5806 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9118 - loss: 57.8820 - val_accuracy: 0.4444 - val_loss: 55.9401 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.6176 - loss: 50.6521 - val_accuracy: 0.5556 - val_loss: 47.9495 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.4706 - loss: 43.2604 - val_accuracy: 0.4444 - val_loss: 42.9712 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7941 - loss: 37.8338 - val_accuracy: 0.5556 - val_loss: 34.8390 - learning_rate: 0.0010\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 825ms/step - accuracy: 0.5000 - loss: 34.8936\n",
      "\n",
      "\n",
      "Sampling frequency of the instance is already 300.0, returning unmodified.\n",
      "Adding metadata with 3 columns\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arazzz/anaconda3/envs/moabb_model_optimization/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.4412 - loss: 74.1248 - val_accuracy: 0.5000 - val_loss: 61.0009 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.6912 - loss: 60.9210 - val_accuracy: 0.6667 - val_loss: 54.8365 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8235 - loss: 54.7095 - val_accuracy: 0.7222 - val_loss: 50.1908 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8676 - loss: 50.0235 - val_accuracy: 0.7222 - val_loss: 46.3987 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9118 - loss: 46.1510 - val_accuracy: 0.5556 - val_loss: 43.1316 - learning_rate: 0.0010\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 502ms/step - accuracy: 0.6818 - loss: 43.2245\n",
      "\n",
      "\n",
      "Adding metadata with 3 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arazzz/anaconda3/envs/moabb_model_optimization/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.3826 - loss: 58.6704 - val_accuracy: 0.4444 - val_loss: 46.8738 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5568 - loss: 45.1231 - val_accuracy: 0.5000 - val_loss: 39.0955 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7461 - loss: 37.7478 - val_accuracy: 0.5556 - val_loss: 33.3310 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8635 - loss: 32.0277 - val_accuracy: 0.6667 - val_loss: 28.6299 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8752 - loss: 27.4983 - val_accuracy: 0.6667 - val_loss: 24.9044 - learning_rate: 0.0010\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 581ms/step - accuracy: 0.7273 - loss: 24.7474\n",
      "\n",
      "\n",
      "Adding metadata with 3 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arazzz/anaconda3/envs/moabb_model_optimization/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.5441 - loss: 69.0628 - val_accuracy: 0.6111 - val_loss: 56.5829 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7794 - loss: 56.3754 - val_accuracy: 0.5000 - val_loss: 49.5106 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.7353 - loss: 48.9271 - val_accuracy: 0.4444 - val_loss: 44.0562 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.7941 - loss: 43.3533 - val_accuracy: 0.5000 - val_loss: 39.5907 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9118 - loss: 38.8609 - val_accuracy: 0.6667 - val_loss: 35.5435 - learning_rate: 0.0010\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 508ms/step - accuracy: 0.6818 - loss: 35.4289\n",
      "\n",
      "\n",
      "Adding metadata with 3 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arazzz/anaconda3/envs/moabb_model_optimization/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.5147 - loss: 17.5704 - val_accuracy: 0.6111 - val_loss: 11.5562 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5735 - loss: 12.0113 - val_accuracy: 0.5556 - val_loss: 11.2002 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.6471 - loss: 11.1829 - val_accuracy: 0.5000 - val_loss: 10.7544 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.6324 - loss: 10.7666 - val_accuracy: 0.5000 - val_loss: 10.3456 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.6471 - loss: 10.2171 - val_accuracy: 0.5000 - val_loss: 9.9621 - learning_rate: 0.0010\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 648ms/step - accuracy: 0.4545 - loss: 9.8732\n",
      "\n",
      "\n",
      "Adding metadata with 3 columns\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arazzz/anaconda3/envs/moabb_model_optimization/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.5441 - loss: 144.6169 - val_accuracy: 0.4444 - val_loss: 122.0854 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.5294 - loss: 122.0523 - val_accuracy: 0.4444 - val_loss: 108.3968 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.7500 - loss: 108.1008 - val_accuracy: 0.3889 - val_loss: 97.9126 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.7206 - loss: 97.7652 - val_accuracy: 0.4444 - val_loss: 90.2073 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.6618 - loss: 89.4070 - val_accuracy: 0.5000 - val_loss: 82.2784 - learning_rate: 0.0010\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 559ms/step - accuracy: 0.5000 - loss: 82.3092\n",
      "\n",
      "\n",
      "Adding metadata with 3 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arazzz/anaconda3/envs/moabb_model_optimization/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.4853 - loss: 38.8051 - val_accuracy: 0.6111 - val_loss: 30.6796 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6618 - loss: 30.5880 - val_accuracy: 0.5556 - val_loss: 28.3745 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6176 - loss: 28.2593 - val_accuracy: 0.5000 - val_loss: 26.6228 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6324 - loss: 26.4686 - val_accuracy: 0.4444 - val_loss: 25.1333 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7647 - loss: 24.9347 - val_accuracy: 0.5000 - val_loss: 23.8282 - learning_rate: 0.0010\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 483ms/step - accuracy: 0.3636 - loss: 23.8184\n",
      "\n",
      "\n",
      "Adding metadata with 3 columns\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arazzz/anaconda3/envs/moabb_model_optimization/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.4706 - loss: 7.6063 - val_accuracy: 0.5556 - val_loss: 6.2018 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7794 - loss: 6.0535 - val_accuracy: 0.5556 - val_loss: 5.6664 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8971 - loss: 5.4858 - val_accuracy: 0.5556 - val_loss: 5.2749 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9706 - loss: 5.0639 - val_accuracy: 0.6111 - val_loss: 4.9481 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9559 - loss: 4.7260 - val_accuracy: 0.6111 - val_loss: 4.6766 - learning_rate: 0.0010\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 624ms/step - accuracy: 0.8182 - loss: 4.6018\n",
      "\n",
      "\n",
      "Adding metadata with 3 columns\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arazzz/anaconda3/envs/moabb_model_optimization/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.4118 - loss: 72.0755 - val_accuracy: 0.4444 - val_loss: 58.4119 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.7353 - loss: 58.2708 - val_accuracy: 0.5556 - val_loss: 50.6149 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6618 - loss: 50.2401 - val_accuracy: 0.4444 - val_loss: 46.0878 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6618 - loss: 44.4053 - val_accuracy: 0.5556 - val_loss: 40.2018 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.7794 - loss: 39.5902 - val_accuracy: 0.4444 - val_loss: 36.6692 - learning_rate: 0.0010\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329ms/step - accuracy: 0.5000 - loss: 36.6747\n",
      "\n",
      "\n",
      "Adding metadata with 3 columns\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arazzz/anaconda3/envs/moabb_model_optimization/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 892ms/step - accuracy: 0.5338 - loss: 95.6195 - val_accuracy: 0.4444 - val_loss: 68.4448 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6820 - loss: 64.4864 - val_accuracy: 0.3333 - val_loss: 51.9365 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5875 - loss: 49.3110 - val_accuracy: 0.3889 - val_loss: 40.7892 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6772 - loss: 38.6940 - val_accuracy: 0.3889 - val_loss: 32.5082 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7808 - loss: 30.6739 - val_accuracy: 0.5556 - val_loss: 25.9664 - learning_rate: 0.0010\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424ms/step - accuracy: 0.5000 - loss: 25.9090\n",
      "\n",
      "\n",
      "Adding metadata with 3 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arazzz/anaconda3/envs/moabb_model_optimization/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.4853 - loss: 138.9495 - val_accuracy: 0.5000 - val_loss: 113.6793 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6912 - loss: 113.5782 - val_accuracy: 0.5000 - val_loss: 100.2464 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6324 - loss: 100.1989 - val_accuracy: 0.5556 - val_loss: 90.1998 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.6618 - loss: 90.1323 - val_accuracy: 0.5556 - val_loss: 82.0555 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.6618 - loss: 81.9803 - val_accuracy: 0.4444 - val_loss: 75.1681 - learning_rate: 0.0010\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 490ms/step - accuracy: 0.6818 - loss: 75.1005\n",
      "\n",
      "\n",
      "Adding metadata with 3 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arazzz/anaconda3/envs/moabb_model_optimization/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.5000 - loss: 126.3484 - val_accuracy: 0.6111 - val_loss: 104.8660 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.5588 - loss: 103.7674 - val_accuracy: 0.4444 - val_loss: 96.6603 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.5441 - loss: 92.5707 - val_accuracy: 0.4444 - val_loss: 86.1050 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.7206 - loss: 79.1065 - val_accuracy: 0.5556 - val_loss: 76.3749 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.6176 - loss: 71.2330 - val_accuracy: 0.4444 - val_loss: 71.5466 - learning_rate: 0.0010\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 718ms/step - accuracy: 0.5000 - loss: 70.5658\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def objective_fn(trial, subjects=[], model_str = \"lstm_cnn_net\", sfreq = 128, batch_size = 128, **kwargs):\n",
    "    _SFREQ_ = sfreq if sfreq else trial.suggest_categorical(\"sfreq\", [128, 256, 300])\n",
    "    _TRAIN_SIZE_ = 0.8\n",
    "    _TEST_SIZE_ = 1 - _TRAIN_SIZE_\n",
    "    _BATCH_SIZE = batch_size if batch_size else trial.suggest_int(\"batch_size\", 32, 256, step=32)\n",
    "\n",
    "    models = {\n",
    "        \"lstm_cnn_net\": lstm_cnn_net,\n",
    "        \"shallow_conv_net\": shallow_conv_net\n",
    "    }\n",
    "    \n",
    "    subjects = subjects if len(subjects) > 0 else fat_dataset.subject_list\n",
    "    model_fn = models[model_str]\n",
    "    \n",
    "    all_channels = fat_dataset.get_data(subjects=[1])[1]['0']['0'].info['ch_names'][:-1]\n",
    "    channels_dict = { k: trial.suggest_categorical(f\"channels_{k}\", [True, False]) for k in all_channels }\n",
    "    channels_idx = [i for i, v in enumerate(channels_dict.values()) if v]\n",
    "    while len(channels_idx) == 0:\n",
    "        channels_dict = { k: trial.suggest_categorical(f\"channels_{k}\", [True, False]) for k in all_channels }\n",
    "        channels_idx = [i for i, v in enumerate(channels_dict.values()) if v]\n",
    "    \n",
    "    X, y, _ = data_generator(fat_dataset, subjects=subjects, channel_idx=channels_idx, sfreq=_SFREQ_)\n",
    "    \n",
    "    _NUM_SAMPLES_ = X.shape[-1]\n",
    "    _NUM_CHANNELS_ = X.shape[-2]\n",
    "    \n",
    "    y_encoded = LabelEncoder().fit_transform(y)\n",
    "    _NUM_CLASSES_ = len(np.unique(y_encoded))\n",
    "    if \"lstm\" in model_str:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X.reshape(-1, _NUM_SAMPLES_, _NUM_CHANNELS_), y_encoded, test_size=_TEST_SIZE_, random_state=SKLRNG, stratify=y_encoded)\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=_TEST_SIZE_, random_state=SKLRNG, shuffle=True, stratify=y_encoded)\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X.reshape(-1, _NUM_SAMPLES_, _NUM_CHANNELS_), y_encoded, test_size=_TEST_SIZE_, random_state=SKLRNG, stratify=y_encoded)\n",
    "        \n",
    "    # y_train = keras.utils.to_categorical(y_train)\n",
    "    # y_test = keras.utils.to_categorical(y_test)\n",
    "    \n",
    "    model_params = {\n",
    "        \"nb_classes\": _NUM_CLASSES_,\n",
    "        \"channels\": _NUM_CHANNELS_,\n",
    "        \"samples\": _NUM_SAMPLES_,\n",
    "    }\n",
    "\n",
    "    if model_str == \"shallow_conv_net\":\n",
    "        model_params = {\n",
    "            **model_params,\n",
    "\n",
    "            \"pool_size_d2\": trial.suggest_int(\"pool_size_d2\", 5, 95, step=5),\n",
    "            \"strides_d2\": trial.suggest_int(\"strides_d2\", 1, 31, step=2),\n",
    "            \"conv_filters_d2\": trial.suggest_int(\"conv_filters_d2\", 5, 55, step=2),\n",
    "            \n",
    "            \"conv2d_1_units\": trial.suggest_int(\"conv2d_1_units\", 10, 200, step=10),\n",
    "            \"conv2d_2_units\": trial.suggest_int(\"conv2d_2_units\", 10, 200, step=10),\n",
    "            \"l2_reg_1\": trial.suggest_float(\"l2_reg_1\", 0.001, 0.8),\n",
    "            \"l2_reg_2\": trial.suggest_float(\"l2_reg_2\", 0.001, 0.8),\n",
    "            \"l2_reg_3\": trial.suggest_float(\"l2_reg_3\", 0.001, 0.8),\n",
    "            \"dropout_rate\": trial.suggest_float(\"dropout_rate\", 0.1, 0.9, step=0.1),\n",
    "        }\n",
    "\n",
    "    if model_str == \"lstm_cnn_net\":\n",
    "        model_params = {\n",
    "            **model_params,\n",
    "\n",
    "            \"conv1d_1_units\": trial.suggest_int(\"conv1d_1_units\", 10, 200, step=10),\n",
    "            \"conv1d_1_kernel_size\": trial.suggest_int(\"conv1d_1_kernel_size\", 1, 100, step=1),\n",
    "            \"conv1d_1_strides\": trial.suggest_int(\"conv1d_1_strides\", 1, 50, step=1),\n",
    "            # \"conv1d_1_maxpool_size\": trial.suggest_int(\"conv1d_1_maxpool_size\", 1, 100, step=2),\n",
    "            \"conv1d_1_maxpool_strides\": trial.suggest_int(\"conv1d_1_maxpool_strides\", 1, 4, step=1),\n",
    "            \"lstm_1_units\": trial.suggest_int(\"lstm_1_units\", 10, 200, step=10),\n",
    "            \"l2_reg_1\": trial.suggest_float(\"l2_reg_1\", 0.001, 0.8),\n",
    "            \"l2_reg_2\": trial.suggest_float(\"l2_reg_2\", 0.001, 0.8),\n",
    "            \"l2_reg_3\": trial.suggest_float(\"l2_reg_3\", 0.001, 0.8),\n",
    "            \"dropout_rate_1\": trial.suggest_float(\"dropout_rate_1\", 0.1, 0.9, step=0.1),\n",
    "            \"dropout_rate_2\": trial.suggest_float(\"dropout_rate_2\", 0.1, 0.9, step=0.1),\n",
    "        }\n",
    "    \n",
    "    model = model_fn(**model_params)\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", \n",
    "        optimizer=\"rmsprop\", \n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")]\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=_BATCH_SIZE,\n",
    "        epochs=5,\n",
    "        validation_split=0.2,\n",
    "        shuffle=True,\n",
    "        callbacks=[\n",
    "            # keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=75, restore_best_weights=True),\n",
    "            # keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=75, factor=0.5)\n",
    "            keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True),\n",
    "            keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=3, factor=0.1)\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Rename history.history[\"sparse_categorical_accuracy\"] to history.history[\"accuracy\"]\n",
    "    # history.history[\"accuracy\"] = history.history[\"sparse_categorical_accuracy\"]\n",
    "    # history.history[\"val_accuracy\"] = history.history[\"val_sparse_categorical_accuracy\"]\n",
    "    \n",
    "    trial.set_user_attr(\"trial_data\", {\n",
    "        \"channels_selected\": np.asarray(fat_dataset.get_data(subjects=[1])[1]['0']['0'].info['ch_names'])[channels_idx],\n",
    "        # \"history\": history,\n",
    "        \"model\": model.to_json(),\n",
    "        \"model_weights\": model.get_weights(),\n",
    "        \"train_accuracy\": history.history[\"accuracy\"],\n",
    "        \"val_accuracy\": history.history[\"val_accuracy\"],\n",
    "        \"test_accuracy\": model.evaluate(X_test, y_test, batch_size=_BATCH_SIZE)[1],\n",
    "    })\n",
    "    \n",
    "    print(\"\\n\")\n",
    "\n",
    "    # cost = np.abs(1 - np.max(history.history[\"val_accuracy\"])) + 1e-5*len(channels_idx) + np.abs(np.mean(np.asarray(history.history[\"accuracy\"]) - np.asarray(history.history[\"val_accuracy\"])))\n",
    "    # cost = np.abs(1 - np.max(history.history[\"val_accuracy\"])) + 0.25*(len(channels_idx)/len(all_channels))\n",
    "    cost = (1 - np.max(history.history[\"val_accuracy\"]))**2 + 0.5*(1 - np.mean(history.history[\"val_accuracy\"]))**2 + 1e-2*len(channels_idx) + 1e-3*np.mean(np.asarray(history.history[\"accuracy\"]) - np.asarray(history.history[\"val_accuracy\"]))**2\n",
    "    # cost = (1 - np.max(history.history[\"val_accuracy\"]))**2 + 1e-5*len(channels_idx) + 1e-3*np.mean(np.asarray(history.history[\"accuracy\"]) - np.asarray(history.history[\"val_accuracy\"]))**2\n",
    "    # cost = (1 - np.max(history.history[\"val_accuracy\"]))**2 + 8e-5*len(channels_idx) #+ 1e-3*(np.mean(history.history[\"accuracy\"][-2:]) - np.mean(history.history[\"val_accuracy\"][-2:]))**2\n",
    "    # cost = (1 - np.max(history.history[\"val_accuracy\"]))**2 + 0.25*(1 - np.mean(history.history[\"val_accuracy\"]))**2 + 0.25*(1 - history.history[\"val_accuracy\"][-1])**2 + 8e-5*len(channels_idx) #+ 1e-3*np.mean(np.asarray(history.history[\"accuracy\"]) - np.asarray(history.history[\"val_accuracy\"]))**2\n",
    "    # cost = (1 - np.max(history.history[\"val_accuracy\"]))**2 + 1e-5*len(channels_idx) + 1e-4*np.mean(np.asarray(history.history[\"accuracy\"]) - np.asarray(history.history[\"val_accuracy\"]))**2 + 1e-5*np.mean(1 - np.max(history.history[\"accuracy\"]))**2\n",
    "    # if not 0.70 <= np.max(history.history[\"accuracy\"]) <= 1.00 or len(channels_idx) < 5:\n",
    "    if not 0.75 <= np.max(history.history[\"accuracy\"]) <= 1.00:\n",
    "        cost += .5\n",
    "\n",
    "    # cost = (1 - np.mean(history.history[\"val_accuracy\"][-2:])) + 0.005*len(channels_idx)\n",
    "    # cost = (1 - np.mean(history.history[\"val_accuracy\"]))**2 + 2.5e-1*(np.mean(history.history[\"accuracy\"]) - np.mean(history.history[\"val_accuracy\"]))**2 + 1.5e-1*(1 - np.max(history.history[\"val_accuracy\"]))**2 + 5e-2*len(channels_idx)\n",
    "    # cost = (1 - np.mean(history.history[\"val_accuracy\"]))**2 + 2.5e-1*(np.mean(history.history[\"accuracy\"]) - np.mean(history.history[\"val_accuracy\"]))**2 + 5e-2*len(channels_idx)\n",
    "    return cost\n",
    "\n",
    "def early_stopping_check(study, trial, early_stopping_rounds=10):\n",
    "    current_trial_number = trial.number\n",
    "    best_trial_number = study.best_trial.number\n",
    "    should_stop = (current_trial_number - best_trial_number) >= early_stopping_rounds\n",
    "    if should_stop:\n",
    "        optuna.logging._get_library_root_logger().info(\"Early stopping detected: %s\", should_stop)\n",
    "        study.stop()\n",
    "\n",
    "def heuristic_optimizer(obj_fn, max_iter = 25, show_progress_bar = True, subjects=[], model_str=\"lstm_cnn_net\", sfreq = 128, batch_size = 128, max_stag_count = 10, **kwargs):\n",
    "    \n",
    "    optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n",
    "\n",
    "    objective = partial(obj_fn, subjects = subjects, model_str = model_str, sfreq = sfreq, batch_size = batch_size)\n",
    "    \n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.NSGAIIISampler())\n",
    "    try:\n",
    "        study.optimize(objective, n_trials=max_iter, show_progress_bar=show_progress_bar, callbacks=[partial(early_stopping_check, early_stopping_rounds=max_stag_count)], **kwargs)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    return study\n",
    "\n",
    "study = heuristic_optimizer(obj_fn = objective_fn, model_str=\"shallow_conv_net\", subjects=[12], sfreq = None, batch_size = None, max_iter = 50, max_stag_count = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>train_val_diff</th>\n",
       "      <th>scores</th>\n",
       "      <th>channels_selected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.189542</td>\n",
       "      <td>0.214396</td>\n",
       "      <td>[F3, Fz, C4, P4, O1, F8, T4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.867647</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.200980</td>\n",
       "      <td>0.285014</td>\n",
       "      <td>[P3, C3, Fz, C4, Cz, Fp2, T5, A2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.808824</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.142157</td>\n",
       "      <td>0.300254</td>\n",
       "      <td>[F3, C4, P4, Cz, Pz, Fp1, Fp2, F7, A2, T4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.955882</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.289216</td>\n",
       "      <td>0.319900</td>\n",
       "      <td>[F3, Fz, F4, C4, P4, Cz, Fp2, O2, F7, F8, T6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.359477</td>\n",
       "      <td>0.320427</td>\n",
       "      <td>[F3, Fz, F4, Pz, Fp1, T3, O2, A2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.808824</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.142157</td>\n",
       "      <td>0.329906</td>\n",
       "      <td>[P3, C3, F3, Fz, P4, Cz, Fp1, T5, F7, A2, T6, T4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.245098</td>\n",
       "      <td>0.334920</td>\n",
       "      <td>[P3, C3, F3, Fz, C4, Fp1, Fp2, T5, O2, F7, F8,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.153595</td>\n",
       "      <td>0.345383</td>\n",
       "      <td>[P4, Pz, T3, T5, O1, O2, A2, T6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.267974</td>\n",
       "      <td>0.391301</td>\n",
       "      <td>[P3, F3, F4, Pz, Fp1, T5, O1, T6, T4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.779412</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.223856</td>\n",
       "      <td>0.418174</td>\n",
       "      <td>[C3, Fz, P4, Fp2, O1, O2, F7, T6, T4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.356209</td>\n",
       "      <td>0.448175</td>\n",
       "      <td>[P3, F3, Fz, P4, Pz, Fp2, T3, T5, O2, F7, F8, T4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.385621</td>\n",
       "      <td>0.448188</td>\n",
       "      <td>[P3, C3, F3, Fz, Pz, Fp1, T5, O1, F7, F8, A2, T4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.459785</td>\n",
       "      <td>[P3, C3, F3, F4, P4, Pz, Fp1, Fp2, T5, O1, O2,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.985294</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.429739</td>\n",
       "      <td>0.475906</td>\n",
       "      <td>[P3, F3, Fz, F4, Cz, Pz, Fp2, T5, O1, O2, F8, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.209150</td>\n",
       "      <td>0.484498</td>\n",
       "      <td>[P3, Fz, P4, Cz, Pz, Fp1, T5, O1, F8, A2, T6, T4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.494360</td>\n",
       "      <td>[P3, C3, P4, Fp2, T3, T5, O1, F8, T4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.720588</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.109477</td>\n",
       "      <td>0.845127</td>\n",
       "      <td>[F3, F4, C4, Cz, Fp1, T3, T5, O1, O2, A2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.720588</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.109477</td>\n",
       "      <td>0.856242</td>\n",
       "      <td>[P3, C3, C4, Cz, Pz, Fp2, A2, T4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.035948</td>\n",
       "      <td>0.900128</td>\n",
       "      <td>[P3, C3, F3, F4, C4, P4, Pz, Fp1, Fp2, T3, F7,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.691176</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.135621</td>\n",
       "      <td>0.907050</td>\n",
       "      <td>[P3, C3, F3, Fz, P4, Fp1, T3, F7, T4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train_acc  test_acc   val_acc  train_val_diff    scores  \\\n",
       "9    0.911765  0.681818  0.722222        0.189542  0.214396   \n",
       "10   0.867647  0.727273  0.666667        0.200980  0.285014   \n",
       "4    0.808824  0.590909  0.666667        0.142157  0.300254   \n",
       "2    0.955882  0.500000  0.666667        0.289216  0.319900   \n",
       "15   0.970588  0.818182  0.611111        0.359477  0.320427   \n",
       "6    0.808824  0.500000  0.666667        0.142157  0.329906   \n",
       "11   0.911765  0.681818  0.666667        0.245098  0.334920   \n",
       "14   0.764706  0.363636  0.611111        0.153595  0.345383   \n",
       "5    0.823529  0.454545  0.555556        0.267974  0.391301   \n",
       "16   0.779412  0.500000  0.555556        0.223856  0.418174   \n",
       "8    0.911765  0.500000  0.555556        0.356209  0.448175   \n",
       "3    0.941176  0.454545  0.555556        0.385621  0.448188   \n",
       "0    0.750000  0.454545  0.555556        0.194444  0.459785   \n",
       "7    0.985294  0.500000  0.555556        0.429739  0.475906   \n",
       "17   0.764706  0.500000  0.555556        0.209150  0.484498   \n",
       "13   0.750000  0.500000  0.500000        0.250000  0.494360   \n",
       "1    0.720588  0.545455  0.611111        0.109477  0.845127   \n",
       "19   0.720588  0.500000  0.611111        0.109477  0.856242   \n",
       "12   0.647059  0.454545  0.611111        0.035948  0.900128   \n",
       "18   0.691176  0.681818  0.555556        0.135621  0.907050   \n",
       "\n",
       "                                    channels_selected  \n",
       "9                        [F3, Fz, C4, P4, O1, F8, T4]  \n",
       "10                  [P3, C3, Fz, C4, Cz, Fp2, T5, A2]  \n",
       "4          [F3, C4, P4, Cz, Pz, Fp1, Fp2, F7, A2, T4]  \n",
       "2       [F3, Fz, F4, C4, P4, Cz, Fp2, O2, F7, F8, T6]  \n",
       "15                  [F3, Fz, F4, Pz, Fp1, T3, O2, A2]  \n",
       "6   [P3, C3, F3, Fz, P4, Cz, Fp1, T5, F7, A2, T6, T4]  \n",
       "11  [P3, C3, F3, Fz, C4, Fp1, Fp2, T5, O2, F7, F8,...  \n",
       "14                   [P4, Pz, T3, T5, O1, O2, A2, T6]  \n",
       "5               [P3, F3, F4, Pz, Fp1, T5, O1, T6, T4]  \n",
       "16              [C3, Fz, P4, Fp2, O1, O2, F7, T6, T4]  \n",
       "8   [P3, F3, Fz, P4, Pz, Fp2, T3, T5, O2, F7, F8, T4]  \n",
       "3   [P3, C3, F3, Fz, Pz, Fp1, T5, O1, F7, F8, A2, T4]  \n",
       "0   [P3, C3, F3, F4, P4, Pz, Fp1, Fp2, T5, O1, O2,...  \n",
       "7   [P3, F3, Fz, F4, Cz, Pz, Fp2, T5, O1, O2, F8, ...  \n",
       "17  [P3, Fz, P4, Cz, Pz, Fp1, T5, O1, F8, A2, T6, T4]  \n",
       "13              [P3, C3, P4, Fp2, T3, T5, O1, F8, T4]  \n",
       "1           [F3, F4, C4, Cz, Fp1, T3, T5, O1, O2, A2]  \n",
       "19                  [P3, C3, C4, Cz, Pz, Fp2, A2, T4]  \n",
       "12  [P3, C3, F3, F4, C4, P4, Pz, Fp1, Fp2, T3, F7,...  \n",
       "18              [P3, C3, F3, Fz, P4, Fp1, T3, F7, T4]  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_metrics_dict = {\n",
    "    \"train_acc\": [],\n",
    "    \"test_acc\": [],\n",
    "    \"val_acc\": [],\n",
    "    \"train_val_diff\": [],\n",
    "    \"scores\": [],\n",
    "    \"channels_selected\": [],\n",
    "}\n",
    "for i, trial in enumerate(study.trials_dataframe().itertuples()):\n",
    "    trial_user_attrs = trial.user_attrs_trial_data\n",
    "    trial_metrics_dict[\"scores\"].append(trial.value)\n",
    "    trial_metrics_dict[\"train_acc\"].append(np.max(trial_user_attrs['train_accuracy']))\n",
    "    trial_metrics_dict[\"test_acc\"].append(trial_user_attrs['test_accuracy'])\n",
    "    trial_metrics_dict[\"val_acc\"].append(np.max(trial_user_attrs['val_accuracy']))\n",
    "    trial_metrics_dict[\"channels_selected\"].append(trial_user_attrs['channels_selected'])\n",
    "    trial_metrics_dict[\"train_val_diff\"].append(abs(np.max(trial_user_attrs['train_accuracy']) - np.max(trial_user_attrs['val_accuracy'])))\n",
    "trial_metrics_df = pd.DataFrame(trial_metrics_dict)\n",
    "# trial_metrics_df.sort_values(\"test_acc\", ascending=False)\n",
    "trial_metrics_df.sort_values(\"scores\", ascending=True)\n",
    "# trial_metrics_df.sort_values(\"train_val_diff\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trial_metrics_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m max_val_acc \u001b[38;5;241m=\u001b[39m trial_metrics_df\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_acc > val_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m)[:\u001b[38;5;241m10\u001b[39m]\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m      2\u001b[0m trial_metrics_df\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_acc > val_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m)[:\u001b[38;5;241m10\u001b[39m]\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_acc == \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_val_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trial_metrics_df' is not defined"
     ]
    }
   ],
   "source": [
    "max_val_acc = trial_metrics_df.sort_values(\"val_acc\", ascending=False).query(\"train_acc > val_acc\")[:10].sort_values('train_acc', ascending=False)[\"val_acc\"].max()\n",
    "trial_metrics_df.sort_values(\"val_acc\", ascending=False).query(\"train_acc > val_acc\")[:10].sort_values('train_acc', ascending=False).query(f\"val_acc == {max_val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>train_val_diff</th>\n",
       "      <th>scores</th>\n",
       "      <th>channels_selected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.189542</td>\n",
       "      <td>0.214396</td>\n",
       "      <td>[F3, Fz, C4, P4, O1, F8, T4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_acc  test_acc   val_acc  train_val_diff    scores  \\\n",
       "9   0.911765  0.681818  0.722222        0.189542  0.214396   \n",
       "\n",
       "              channels_selected  \n",
       "9  [F3, Fz, C4, P4, O1, F8, T4]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'sfreq'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">300</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">192</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'pool_size_d2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'strides_d2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'conv_filters_d2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'conv2d_1_units'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">130</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'conv2d_2_units'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'l2_reg_1'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6220290148457239</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'l2_reg_2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4018286124051849</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'l2_reg_3'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5698377202461229</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'dropout_rate'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'sfreq'\u001b[0m: \u001b[1;36m300\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'batch_size'\u001b[0m: \u001b[1;36m192\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'pool_size_d2'\u001b[0m: \u001b[1;36m50\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'strides_d2'\u001b[0m: \u001b[1;36m19\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'conv_filters_d2'\u001b[0m: \u001b[1;36m35\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'conv2d_1_units'\u001b[0m: \u001b[1;36m130\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'conv2d_2_units'\u001b[0m: \u001b[1;36m60\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'l2_reg_1'\u001b[0m: \u001b[1;36m0.6220290148457239\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'l2_reg_2'\u001b[0m: \u001b[1;36m0.4018286124051849\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'l2_reg_3'\u001b[0m: \u001b[1;36m0.5698377202461229\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'dropout_rate'\u001b[0m: \u001b[1;36m0.6\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">test_accuracy = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6818181872367859</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "test_accuracy = \u001b[1;36m0.6818181872367859\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">val_accuracy = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7222222089767456</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "val_accuracy = \u001b[1;36m0.7222222089767456\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">channels_selected = <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'F3'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'Fz'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'C4'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'P4'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'O1'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'F8'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'T4'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "channels_selected = \u001b[1m[\u001b[0m\u001b[32m'F3'\u001b[0m \u001b[32m'Fz'\u001b[0m \u001b[32m'C4'\u001b[0m \u001b[32m'P4'\u001b[0m \u001b[32m'O1'\u001b[0m \u001b[32m'F8'\u001b[0m \u001b[32m'T4'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_val_acc = trial_metrics_df.sort_values(\"val_acc\", ascending=False).query(\"train_acc > val_acc\")[:10].sort_values('train_acc', ascending=False)[\"val_acc\"].max()\n",
    "# best_candidate = trial_metrics_df.sort_values(\"scores\", ascending=False).query(f\"val_acc == {max_val_acc}\").sort_values('train_acc', ascending=False)\n",
    "# best_candidate = trial_metrics_df.sort_values(\"val_acc\", ascending=False).query(\"train_acc > val_acc\")[:10].sort_values('train_acc', ascending=False).query(f\"val_acc == {max_val_acc}\").sort_values(\"train_val_diff\", ascending=True)\n",
    "best_candidate = trial_metrics_df.sort_values(\"val_acc\", ascending=False).query(\"train_acc > val_acc\")[:10].sort_values('train_acc', ascending=False).query(f\"val_acc == {max_val_acc}\").query(\"train_acc >= 0.7\").sort_values(\"train_val_diff\", ascending=True)\n",
    "best_candidate = best_candidate.sort_values(\"train_acc\", ascending=False)\n",
    "display(best_candidate)\n",
    "best_candidate_trial_id = best_candidate.index[0]\n",
    "\n",
    "rpprint({ k: v for k, v in study.best_trial.params.items() if not k.startswith(\"channels\") })\n",
    "rprint(\"test_accuracy =\", study.trials[best_candidate_trial_id].user_attrs[\"trial_data\"][\"test_accuracy\"])\n",
    "rprint(\"val_accuracy =\", np.max(study.trials[best_candidate_trial_id].user_attrs[\"trial_data\"][\"val_accuracy\"]))\n",
    "rprint(\"channels_selected =\", study.trials[best_candidate_trial_id].user_attrs[\"trial_data\"][\"channels_selected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_val_acc = trial_metrics_df.sort_values(\"train_val_diff\", ascending=True).query(\"train_acc >= val_acc\")[:15][\"val_acc\"].max()\n",
    "# # max_val_acc = trial_metrics_df.sort_values(\"train_val_diff\", ascending=True)[:15][\"val_acc\"].max()\n",
    "# best_candidate = trial_metrics_df.sort_values(\"train_acc\", ascending=False).query(f\"val_acc == {max_val_acc}\")\n",
    "# display(best_candidate)\n",
    "# best_candidate_trial_id = best_candidate.index[0]\n",
    "\n",
    "# rpprint({ k: v for k, v in study.best_trial.params.items() if not k.startswith(\"channels\") })\n",
    "# rprint(\"test_accuracy =\", study.trials[best_candidate_trial_id].user_attrs[\"trial_data\"][\"test_accuracy\"])\n",
    "# rprint(\"val_accuracy =\", np.max(study.trials[best_candidate_trial_id].user_attrs[\"trial_data\"][\"val_accuracy\"]))\n",
    "# rprint(\"channels_selected =\", study.trials[best_candidate_trial_id].user_attrs[\"trial_data\"][\"channels_selected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arazzz/anaconda3/envs/moabb_model_optimization/lib/python3.11/site-packages/numpy/lib/npyio.py:521: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n"
     ]
    }
   ],
   "source": [
    "# Save to disk\n",
    "subject = 12\n",
    "temp_id = uuid.uuid4()\n",
    "np.save(f\"./best_subjects_search_results/subject_{subject}_12_shallow_conv_net_{temp_id}.npy\", study.trials[best_candidate_trial_id])\n",
    "np.save(f\"./best_subjects_search_results/weights/subject_{subject}_12_shallow_conv_net_{temp_id}__weights.npy\", study.trials[best_candidate_trial_id].user_attrs[\"trial_data\"][\"model_weights\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'sfreq'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">300</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">192</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'pool_size_d2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'strides_d2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'conv_filters_d2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'conv2d_1_units'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">130</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'conv2d_2_units'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'l2_reg_1'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6220290148457239</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'l2_reg_2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4018286124051849</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'l2_reg_3'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5698377202461229</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'dropout_rate'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'sfreq'\u001b[0m: \u001b[1;36m300\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'batch_size'\u001b[0m: \u001b[1;36m192\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'pool_size_d2'\u001b[0m: \u001b[1;36m50\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'strides_d2'\u001b[0m: \u001b[1;36m19\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'conv_filters_d2'\u001b[0m: \u001b[1;36m35\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'conv2d_1_units'\u001b[0m: \u001b[1;36m130\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'conv2d_2_units'\u001b[0m: \u001b[1;36m60\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'l2_reg_1'\u001b[0m: \u001b[1;36m0.6220290148457239\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'l2_reg_2'\u001b[0m: \u001b[1;36m0.4018286124051849\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'l2_reg_3'\u001b[0m: \u001b[1;36m0.5698377202461229\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'dropout_rate'\u001b[0m: \u001b[1;36m0.6\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">test_accuracy = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6818181872367859</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "test_accuracy = \u001b[1;36m0.6818181872367859\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">val_accuracy = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7222222089767456</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "val_accuracy = \u001b[1;36m0.7222222089767456\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">channels_selected = <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'F3'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'Fz'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'C4'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'P4'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'O1'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'F8'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'T4'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "channels_selected = \u001b[1m[\u001b[0m\u001b[32m'F3'\u001b[0m \u001b[32m'Fz'\u001b[0m \u001b[32m'C4'\u001b[0m \u001b[32m'P4'\u001b[0m \u001b[32m'O1'\u001b[0m \u001b[32m'F8'\u001b[0m \u001b[32m'T4'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rpprint({ k: v for k, v in study.best_trial.params.items() if not k.startswith(\"channels\") })\n",
    "rprint(\"test_accuracy =\", study.best_trial.user_attrs[\"trial_data\"][\"test_accuracy\"])\n",
    "rprint(\"val_accuracy =\", np.max(study.best_trial.user_attrs[\"trial_data\"][\"val_accuracy\"]))\n",
    "rprint(\"channels_selected =\", study.best_trial.user_attrs[\"trial_data\"][\"channels_selected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'sfreq'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">300</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">192</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'pool_size_d2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'strides_d2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'conv_filters_d2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'conv2d_1_units'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">130</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'conv2d_2_units'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'l2_reg_1'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6220290148457239</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'l2_reg_2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4018286124051849</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'l2_reg_3'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5698377202461229</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'dropout_rate'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'sfreq'\u001b[0m: \u001b[1;36m300\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'batch_size'\u001b[0m: \u001b[1;36m192\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'pool_size_d2'\u001b[0m: \u001b[1;36m50\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'strides_d2'\u001b[0m: \u001b[1;36m19\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'conv_filters_d2'\u001b[0m: \u001b[1;36m35\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'conv2d_1_units'\u001b[0m: \u001b[1;36m130\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'conv2d_2_units'\u001b[0m: \u001b[1;36m60\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'l2_reg_1'\u001b[0m: \u001b[1;36m0.6220290148457239\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'l2_reg_2'\u001b[0m: \u001b[1;36m0.4018286124051849\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'l2_reg_3'\u001b[0m: \u001b[1;36m0.5698377202461229\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'dropout_rate'\u001b[0m: \u001b[1;36m0.6\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">test_accuracy = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6818181872367859</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "test_accuracy = \u001b[1;36m0.6818181872367859\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">val_accuracy = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7222222089767456</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "val_accuracy = \u001b[1;36m0.7222222089767456\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">channels_selected = <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'F3'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'Fz'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'C4'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'P4'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'O1'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'F8'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'T4'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "channels_selected = \u001b[1m[\u001b[0m\u001b[32m'F3'\u001b[0m \u001b[32m'Fz'\u001b[0m \u001b[32m'C4'\u001b[0m \u001b[32m'P4'\u001b[0m \u001b[32m'O1'\u001b[0m \u001b[32m'F8'\u001b[0m \u001b[32m'T4'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding metadata with 3 columns\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unable to automatically build the model. Please build it yourself before calling fit/evaluate/predict. A model is 'built' when its variables have been created and its `self.built` attribute is True. Usually, calling the model on a batch of data is the right way to build it.\nException encountered:\n'Input 0 of layer \"functional_191\" is incompatible with the layer: expected shape=(None, 7, 601, 1), found shape=(32, 7, 256)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m y_encoded \u001b[38;5;241m=\u001b[39m LabelEncoder()\u001b[38;5;241m.\u001b[39mfit_transform(y)\n\u001b[1;32m     27\u001b[0m subject_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 28\u001b[0m subject_model\u001b[38;5;241m.\u001b[39mevaluate(X, y_encoded)\n",
      "File \u001b[0;32m~/anaconda3/envs/moabb_model_optimization/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/moabb_model_optimization/lib/python3.11/site-packages/keras/src/trainers/trainer.py:923\u001b[0m, in \u001b[0;36mTrainer._symbolic_build\u001b[0;34m(self, iterator, data_batch)\u001b[0m\n\u001b[1;32m    921\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mcompute_output_spec(\u001b[38;5;28mself\u001b[39m, x)\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 923\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    924\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to automatically build the model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease build it yourself before calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit/evaluate/predict. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    927\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA model is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuilt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m when its variables have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    928\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeen created and its `self.built` attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    929\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis True. Usually, calling the model on a batch \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    930\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof data is the right way to build it.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    931\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException encountered:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    932\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    933\u001b[0m     )\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compile_metrics_unbuilt:\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;66;03m# Build all metric state with `backend.compute_output_spec`.\u001b[39;00m\n\u001b[1;32m    936\u001b[0m     backend\u001b[38;5;241m.\u001b[39mcompute_output_spec(\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics,\n\u001b[1;32m    938\u001b[0m         x,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    941\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m    942\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to automatically build the model. Please build it yourself before calling fit/evaluate/predict. A model is 'built' when its variables have been created and its `self.built` attribute is True. Usually, calling the model on a batch of data is the right way to build it.\nException encountered:\n'Input 0 of layer \"functional_191\" is incompatible with the layer: expected shape=(None, 7, 601, 1), found shape=(32, 7, 256)'"
     ]
    }
   ],
   "source": [
    "subject_data = study.trials[best_candidate_trial_id]\n",
    "subject_trial_data = subject_data.user_attrs['trial_data']\n",
    "subject_model_json = subject_trial_data['model']\n",
    "subject_model = keras.models.model_from_json(subject_model_json, custom_objects=CUSTOM_OBJECTS)\n",
    "subject_model_weights = subject_trial_data['model_weights']\n",
    "subject_model.set_weights(subject_model_weights)\n",
    "\n",
    "def channels_to_channels_idx(channels, dataset):\n",
    "    all_channels = dataset.get_data(subjects=[1])[1]['0']['0'].info['ch_names'][:-1]\n",
    "    channels_dict = { k: (True if k in channels else False) for k in all_channels }\n",
    "    channels_idx = [i for i, v in enumerate(channels_dict.values()) if v]\n",
    "    return channels_idx\n",
    "\n",
    "subject_channels_idx = channels_to_channels_idx(subject_trial_data['channels_selected'], fat_dataset)\n",
    "subject_sfreq = subject_trial_data['sfreq'] if 'sfreq' in subject_trial_data else 128\n",
    "subject_data_hyper_params = { k: v for k, v in subject_data.params.items() if not k.startswith('channels_') }\n",
    "subject_batch_size = subject_data.params['batch_size']\n",
    "\n",
    "rpprint({ k: v for k, v in subject_data.params.items() if not k.startswith(\"channels\") })\n",
    "rprint(\"test_accuracy =\", subject_data.user_attrs[\"trial_data\"][\"test_accuracy\"])\n",
    "rprint(\"val_accuracy =\", np.max(subject_data.user_attrs[\"trial_data\"][\"val_accuracy\"]))\n",
    "rprint(\"channels_selected =\", subject_data.user_attrs[\"trial_data\"][\"channels_selected\"])\n",
    "\n",
    "X, y, _ = data_generator(fat_dataset, subjects=[12], channel_idx=subject_channels_idx, sfreq=subject_sfreq)\n",
    "y_encoded = LabelEncoder().fit_transform(y)\n",
    "\n",
    "subject_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "subject_model.evaluate(X, y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# channels = fat_dataset.get_data(subjects=[1])[1]['0']['0'].info['ch_names'][:-1]\n",
    "# channels_of_interest = ['C3', 'F3', 'C4', 'T5', 'O1', 'O2', 'A2', 'T6']\n",
    "# channels_of_interest_idx = np.where(np.isin(channels, channels_of_interest))[0]\n",
    "\n",
    "# channels_of_interest_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model/pipeline hyperparameter optimization test\n",
    "data_store_dict = {}\n",
    "\n",
    "def objective_fn_lstm_cnn_net(trial, **kwargs):\n",
    "    # _SFREQ_ = trial.suggest_categorical(\"sfreq\", [128, 250])\n",
    "    _SFREQ_ = 300\n",
    "    # _TRAIN_SIZE_ = trial.suggest_categorical(\"train_size\", [0.8, 0.9])\n",
    "    _TRAIN_SIZE_ = 0.8\n",
    "    _TEST_SIZE_ = 1 - _TRAIN_SIZE_\n",
    "    _BATCH_SIZE = 256\n",
    "    \n",
    "    channels_dict = { k: trial.suggest_categorical(f\"channels_{k}\", [True, False]) for k in fat_dataset.get_data(subjects=[1])[1]['0']['0'].info['ch_names'][:-1] }\n",
    "    channels_idx = [i for i, v in enumerate(channels_dict.values()) if v]\n",
    "    while len(channels_idx) == 0:\n",
    "        channels_dict = { k: trial.suggest_categorical(f\"channels_{k}\", [True, False]) for k in fat_dataset.get_data(subjects=[1])[1]['0']['0'].info['ch_names'][:-1] }\n",
    "        channels_idx = [i for i, v in enumerate(channels_dict.values()) if v]\n",
    "    \n",
    "    X, y, _ = data_generator(fat_dataset, subjects=[6], channel_idx=channels_idx, sfreq=_SFREQ_)\n",
    "    \n",
    "    _NUM_SAMPLES_ = X.shape[-1]\n",
    "    _NUM_CHANNELS_ = X.shape[-2]\n",
    "    # rpprint({\n",
    "    #     \"NUM_SAMPLES\": _NUM_SAMPLES_,\n",
    "    #     \"NUM_CHANNELS\": _NUM_CHANNELS_,\n",
    "    #     \"X\": X.shape,\n",
    "    #     \"y\": y.shape,\n",
    "    #     \"channels selected\": np.asarray(fat_dataset.get_data(subjects=[1])[1]['0']['0'].info['ch_names'])[channels_idx]\n",
    "    # })\n",
    "    y_encoded = LabelEncoder().fit_transform(y)\n",
    "    _NUM_CLASSES_ = len(np.unique(y_encoded))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X.reshape(-1, _NUM_SAMPLES_, _NUM_CHANNELS_), y_encoded, test_size=_TEST_SIZE_, random_state=SKLRNG, stratify=y_encoded)\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=_TEST_SIZE_, random_state=SKLRNG, stratify=y_encoded)\n",
    "    \n",
    "    model_params = {\n",
    "        \"nb_classes\": _NUM_CLASSES_,\n",
    "        \"channels\": _NUM_CHANNELS_,\n",
    "        \"samples\": _NUM_SAMPLES_,\n",
    "    }\n",
    "    \n",
    "    model = lstm_cnn_net_v2(**model_params)\n",
    "    # model = lstm_cnn_net(**model_params)\n",
    "    # model = shallow_conv_net(**model_params)\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", \n",
    "        optimizer=\"adam\", \n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=_BATCH_SIZE,\n",
    "        epochs=25,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[\n",
    "            # keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=75, restore_best_weights=True),\n",
    "            # keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=75, factor=0.5)\n",
    "            keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True),\n",
    "            keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=3, factor=0.1)\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    trial.set_user_attr(\"trial_data\", {\n",
    "        \"channels_selected\": np.asarray(fat_dataset.get_data(subjects=[1])[1]['0']['0'].info['ch_names'])[channels_idx],\n",
    "        # \"history\": history,\n",
    "        # \"model\": model,\n",
    "        \"val_accuracy\": history.history[\"val_accuracy\"],\n",
    "        \"test_accuracy\": model.evaluate(X_test, y_test, batch_size=_BATCH_SIZE)[1]\n",
    "    })\n",
    "\n",
    "    # return (1 - np.mean(history.history[\"val_accuracy\"][-2:])) + 0.005*len(channels_idx)\n",
    "    return (1 - history.history[\"val_accuracy\"][-1])**2 + 0.00005*len(channels_idx)\n",
    "\n",
    "def heuristic_optimizer(obj_fn = lambda: None, max_iter = 15, show_progress_bar = True, **kwargs):\n",
    "    \n",
    "    optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n",
    "\n",
    "    # trial_data = []\n",
    "    # trial_callback = lambda study, trial: trial_data.append(trial.user_attrs[\"trial_data\"])\n",
    "    \n",
    "    # study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.CmaEsSampler())\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.NSGAIISampler())\n",
    "    study.optimize(obj_fn, n_trials=max_iter, show_progress_bar=show_progress_bar, callbacks=[], **kwargs)\n",
    "    \n",
    "    return study\n",
    "\n",
    "study = heuristic_optimizer(obj_fn = objective_fn_lstm_cnn_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'sfreq'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">300</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">192</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'pool_size_d2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'strides_d2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'conv_filters_d2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'conv2d_1_units'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'conv2d_2_units'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'l2_reg_1'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.28652535852744876</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'l2_reg_2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7133882727980291</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'l2_reg_3'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4398302165849516</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'dropout_rate'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'sfreq'\u001b[0m: \u001b[1;36m300\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'batch_size'\u001b[0m: \u001b[1;36m192\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'pool_size_d2'\u001b[0m: \u001b[1;36m5\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'strides_d2'\u001b[0m: \u001b[1;36m3\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'conv_filters_d2'\u001b[0m: \u001b[1;36m35\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'conv2d_1_units'\u001b[0m: \u001b[1;36m80\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'conv2d_2_units'\u001b[0m: \u001b[1;36m40\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'l2_reg_1'\u001b[0m: \u001b[1;36m0.28652535852744876\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'l2_reg_2'\u001b[0m: \u001b[1;36m0.7133882727980291\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'l2_reg_3'\u001b[0m: \u001b[1;36m0.4398302165849516\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'dropout_rate'\u001b[0m: \u001b[1;36m0.6\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">test_accuracy = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "test_accuracy = \u001b[1;36m0.5\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">val_accuracy = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7222222089767456</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "val_accuracy = \u001b[1;36m0.7222222089767456\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">channels_selected = <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'C3'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'F3'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'Cz'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'Fp1'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'Fp2'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'T3'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'O2'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "channels_selected = \u001b[1m[\u001b[0m\u001b[32m'C3'\u001b[0m \u001b[32m'F3'\u001b[0m \u001b[32m'Cz'\u001b[0m \u001b[32m'Fp1'\u001b[0m \u001b[32m'Fp2'\u001b[0m \u001b[32m'T3'\u001b[0m \u001b[32m'O2'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rpprint({ k: v for k, v in study.best_trial.params.items() if not k.startswith(\"channels\") })\n",
    "rprint(\"test_accuracy =\", study.best_trial.user_attrs[\"trial_data\"][\"test_accuracy\"])\n",
    "rprint(\"val_accuracy =\", np.max(study.best_trial.user_attrs[\"trial_data\"][\"val_accuracy\"]))\n",
    "rprint(\"channels_selected =\", study.best_trial.user_attrs[\"trial_data\"][\"channels_selected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(\"model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from disk\n",
    "loaded_model = keras.saving.load_model(\"model.keras\", custom_objects=CUSTOM_OBJECTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pruning and quantization test\n",
    "\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_wrapper\n",
    "\n",
    "baseline_model = loaded_model\n",
    "pruning_params = {\n",
    "    'pruning_schedule': pruning_wrapper.PolynomialDecay(initial_sparsity=0.00,\n",
    "                                                        final_sparsity=0.80,\n",
    "                                                        begin_step=0,\n",
    "                                                        end_step=100)\n",
    "}\n",
    "sparse_model = pruning_wrapper.PruneLowMagnitude(baseline_model, **pruning_params)\n",
    "\n",
    "sparse_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectrogram(epoch_data):\n",
    "    spectrograms = []\n",
    "    for epoch in epoch_data:\n",
    "        epoch_spectrograms = []\n",
    "        for channel in epoch:\n",
    "            f, t, Sxx = sp.signal.spectrogram(channel, fs=50)\n",
    "            epoch_spectrograms.append(Sxx)\n",
    "        spectrograms.append(np.array(epoch_spectrograms))\n",
    "    return np.array(spectrograms)\n",
    "\n",
    "spectograms = create_spectrogram(X)\n",
    "rprint(spectograms.shape) # (180, 21, 129, 2)\n",
    "\n",
    "num_left_hand_epochs_to_visualize = 2\n",
    "num_right_hand_epochs_to_visualize = 2\n",
    "\n",
    "# Select epochs for visualization\n",
    "left_hand_epochs_to_visualize = np.where((y == 'left_hand'))[0]\n",
    "right_hand_epochs_to_visualize = np.where((y == 'right_hand'))[0]\n",
    "left_hand_epochs_to_visualize = np.random.choice(left_hand_epochs_to_visualize, num_left_hand_epochs_to_visualize)\n",
    "right_hand_epochs_to_visualize = np.random.choice(right_hand_epochs_to_visualize, num_right_hand_epochs_to_visualize)\n",
    "epochs_to_visualize = np.concatenate((left_hand_epochs_to_visualize, right_hand_epochs_to_visualize))\n",
    "\n",
    "rprint(epochs_to_visualize)\n",
    "\n",
    "# Visualize spectrograms\n",
    "fig, axs = plt.subplots(epochs_to_visualize.size, 1, figsize=(10, 10))\n",
    "for i, idx in enumerate(epochs_to_visualize):\n",
    "    axs[i].matshow(spectograms[idx, :, :, 0], cmap='viridis')\n",
    "    axs[i].set_title(y[idx])\n",
    "    axs[i].set_xlabel('Frequency (Hz)')\n",
    "    axs[i].set_ylabel('Time (s)')\n",
    "    axs[i].set_xlim(0, 60) # Limit x-axis to 60Hz\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
