{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# region General Imports\n",
    "import os\n",
    "import uuid\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import glob\n",
    "import pickle\n",
    "import tqdm\n",
    "import copy\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mne\n",
    "from rich import print as rprint\n",
    "from rich.pretty import pprint as rpprint\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "# endregion General Imports\n",
    "\n",
    "from model_optim.model_optimizer import ModelOptimizer\n",
    "\n",
    "# Dataset\n",
    "from custom_datasets.fatigue_mi import FatigueMI\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'sfreq'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">300</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">160</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'val_acc'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8333333134651184</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'test_acc'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8636363744735718</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'channels'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">array</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008000; text-decoration-color: #008000\">'Fz'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'C4'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Fp2'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'T5'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'O2'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'F7'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'F8'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'A2'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'T6'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'T4'</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'&lt;U3'</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'sfreq'\u001b[0m: \u001b[1;36m300\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'batch_size'\u001b[0m: \u001b[1;36m160\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'val_acc'\u001b[0m: \u001b[1;36m0.8333333134651184\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'test_acc'\u001b[0m: \u001b[1;36m0.8636363744735718\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'channels'\u001b[0m: \u001b[1;35marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[32m'Fz'\u001b[0m, \u001b[32m'C4'\u001b[0m, \u001b[32m'Fp2'\u001b[0m, \u001b[32m'T5'\u001b[0m, \u001b[32m'O2'\u001b[0m, \u001b[32m'F7'\u001b[0m, \u001b[32m'F8'\u001b[0m, \u001b[32m'A2'\u001b[0m, \u001b[32m'T6'\u001b[0m, \u001b[32m'T4'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│     \u001b[0m\u001b[33mdtype\u001b[0m=\u001b[32m'<U3'\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# np.load(\"./bkup/best_subjects_search_results/subject_6_shallow_conv_net_7b71b3b6-52f7-435c-be85-2cb99e793f98.npy\", allow_pickle=True).item().user_attrs['trial_data']\n",
    "\n",
    "prev_best_full = np.load(\"./_temp/[1]/292cbc92b8cf46da9986fe7d8447819f/model/study_best_trial.npy\", allow_pickle=True).item()\n",
    "prev_best = prev_best_full.user_attrs['trial_data']\n",
    "\n",
    "rpprint({\n",
    "    \"sfreq\": prev_best_full.params[\"sfreq\"],\n",
    "    \"batch_size\": prev_best_full.params[\"batch_size\"],\n",
    "    \"val_acc\": np.max(prev_best[\"val_accuracy\"]),\n",
    "    \"test_acc\": prev_best[\"test_accuracy\"],\n",
    "    \"channels\": prev_best[\"channels_selected\"],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optimizer = ModelOptimizer(\n",
    "    dataset=FatigueMI(),\n",
    "    model_name=\"shallow_conv_net\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arazzz/Documents/GitHub/moabb_model_optimization/model_optimization_tests/model_optim/model_optimizer.py:399: ExperimentalWarning: NSGAIIISampler is experimental (supported from v3.2.0). The interface can change in the future.\n",
      "  direction=\"minimize\", sampler=optuna.samplers.NSGAIIISampler()\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding metadata with 3 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 15:42:09.106436: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-07 15:42:09.366020: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-07 15:42:09.366155: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-07 15:42:09.368641: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-07 15:42:09.368765: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-07 15:42:09.368812: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-07 15:42:12.362898: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-07 15:42:12.363002: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-07 15:42:12.363013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-03-07 15:42:12.363069: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-07 15:42:12.363139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6593 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 15:42:13.497839: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2024-03-07 15:42:14.862613: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2024-03-07 15:42:16.074736: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-03-07 15:42:16.202945: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.79GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-07 15:42:16.242513: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.79GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-07 15:42:16.522017: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-03-07 15:42:17.636870: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fef740170d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-07 15:42:17.636914: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1070, Compute Capability 6.1\n",
      "2024-03-07 15:42:17.642515: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1709851337.710822  445323 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2024-03-07 15:42:18.187736: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.46GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-07 15:42:18.220344: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.46GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-07 15:42:18.243974: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.38GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-07 15:42:18.272490: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.38GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - ETA: 0s - loss: 41.5754 - accuracy: 0.4706\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.55556, storing weights.\n",
      "1/1 [==============================] - 6s 6s/step - loss: 41.5754 - accuracy: 0.4706 - val_loss: 39.4488 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - ETA: 0s - loss: 39.2881 - accuracy: 0.5588\n",
      "Epoch 00002: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 39.2881 - accuracy: 0.5588 - val_loss: 37.4198 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - ETA: 0s - loss: 37.3212 - accuracy: 0.8529\n",
      "Epoch 00003: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 37.3212 - accuracy: 0.8529 - val_loss: 35.5479 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - ETA: 0s - loss: 35.4188 - accuracy: 0.8382\n",
      "Epoch 00004: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 35.4188 - accuracy: 0.8382 - val_loss: 33.7542 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - ETA: 0s - loss: 33.5831 - accuracy: 0.8971\n",
      "Epoch 00005: val_accuracy improved from 0.55556 to 0.61111, storing weights.\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 33.5831 - accuracy: 0.8971 - val_loss: 32.0229 - val_accuracy: 0.6111 - lr: 0.0010\n",
      "Using epoch 00005 with val_accuracy: 0.61111\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 371ms/step - loss: 32.0862 - accuracy: 0.4545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.151685:   2%|▏         | 1/50 [00:11<09:18, 11.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Adding metadata with 3 columns\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 15:42:21.326929: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.05GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-07 15:42:21.356575: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.05GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - ETA: 0s - loss: 82.7216 - accuracy: 0.5000\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.44444, storing weights.\n",
      "1/1 [==============================] - 2s 2s/step - loss: 82.7216 - accuracy: 0.5000 - val_loss: 78.6943 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - ETA: 0s - loss: 78.7419 - accuracy: 0.6176\n",
      "Epoch 00002: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 78.7419 - accuracy: 0.6176 - val_loss: 74.3647 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - ETA: 0s - loss: 73.7200 - accuracy: 0.9706\n",
      "Epoch 00003: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 73.7200 - accuracy: 0.9706 - val_loss: 70.9633 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - ETA: 0s - loss: 69.9665 - accuracy: 0.8088\n",
      "Epoch 00004: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 69.9665 - accuracy: 0.8088 - val_loss: 66.8951 - val_accuracy: 0.3889 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - ETA: 0s - loss: 65.8714 - accuracy: 1.0000\n",
      "Epoch 00005: val_accuracy improved from 0.44444 to 0.50000, storing weights.\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 65.8714 - accuracy: 1.0000 - val_loss: 63.1890 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Using epoch 00005 with val_accuracy: 0.50000\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 62.8128 - accuracy: 0.6818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.151685:   4%|▍         | 2/50 [00:13<04:58,  6.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sampling frequency of the instance is already 300.0, returning unmodified.\n",
      "Adding metadata with 3 columns\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - ETA: 0s - loss: 7.8184 - accuracy: 0.5735\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.50000, storing weights.\n",
      "1/1 [==============================] - 2s 2s/step - loss: 7.8184 - accuracy: 0.5735 - val_loss: 6.1227 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - ETA: 0s - loss: 6.1800 - accuracy: 0.6029\n",
      "Epoch 00002: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 6.1800 - accuracy: 0.6029 - val_loss: 6.0463 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - ETA: 0s - loss: 5.9785 - accuracy: 0.6765\n",
      "Epoch 00003: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 5.9785 - accuracy: 0.6765 - val_loss: 5.9670 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - ETA: 0s - loss: 5.7551 - accuracy: 0.7647\n",
      "Epoch 00004: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 5.7551 - accuracy: 0.7647 - val_loss: 5.8311 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - ETA: 0s - loss: 5.5760 - accuracy: 0.8382\n",
      "Epoch 00005: val_accuracy improved from 0.50000 to 0.55556, storing weights.\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 5.5760 - accuracy: 0.8382 - val_loss: 5.6840 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Using epoch 00005 with val_accuracy: 0.55556\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 5.8168 - accuracy: 0.4545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.151685:   6%|▌         | 3/50 [00:16<03:29,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sampling frequency of the instance is already 300.0, returning unmodified.\n",
      "Adding metadata with 3 columns\n",
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "study = model_optimizer.search_best_model(\n",
    "    subjects=[1],\n",
    "    # channels=['Fz', 'C4', 'Fp2', 'T5', 'O2', 'F7', 'F8', 'A2', 'T6', 'T4'],\n",
    "    # sfreq=300,\n",
    "    # batch_size=160,\n",
    "    max_iter=50,\n",
    "    max_epochs=5,\n",
    "    max_stag_count=25,\n",
    "    rounds=1,\n",
    "    replace_previous_study_for_subjects=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>train_val_acc_diff</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>train_val_loss_diff</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>scores</th>\n",
       "      <th>channels_selected</th>\n",
       "      <th>sfreq</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.897059</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.119281</td>\n",
       "      <td>21.552662</td>\n",
       "      <td>20.884737</td>\n",
       "      <td>0.667925</td>\n",
       "      <td>24.046930</td>\n",
       "      <td>0.049428</td>\n",
       "      <td>[C3, F4, Cz, Fp1, Fp2, T5, F7, A2, T6]</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.779412</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.057190</td>\n",
       "      <td>17.909790</td>\n",
       "      <td>16.320948</td>\n",
       "      <td>1.588842</td>\n",
       "      <td>16.539864</td>\n",
       "      <td>0.077211</td>\n",
       "      <td>[P4, Fp1, Fp2, T5, O2, F7, F8, A2, T6, T4]</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.779412</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.112745</td>\n",
       "      <td>12.912975</td>\n",
       "      <td>12.636445</td>\n",
       "      <td>0.276530</td>\n",
       "      <td>12.697705</td>\n",
       "      <td>0.111136</td>\n",
       "      <td>[P3, Fz, Cz, O2, F8]</td>\n",
       "      <td>256</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.799840</td>\n",
       "      <td>4.123840</td>\n",
       "      <td>0.324001</td>\n",
       "      <td>8.590641</td>\n",
       "      <td>0.111161</td>\n",
       "      <td>[P3, F3, F4, C4, Cz, Fp1, O1, F7, F8, A2]</td>\n",
       "      <td>300</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>13.156047</td>\n",
       "      <td>12.944667</td>\n",
       "      <td>0.211380</td>\n",
       "      <td>13.696197</td>\n",
       "      <td>0.111161</td>\n",
       "      <td>[C3, F4, C4, Pz, Fp1, T5, F7, A2, T6, T4]</td>\n",
       "      <td>300</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>50.000744</td>\n",
       "      <td>46.522675</td>\n",
       "      <td>3.478069</td>\n",
       "      <td>64.703026</td>\n",
       "      <td>0.111166</td>\n",
       "      <td>[P3, C3, F4, P4, Cz, Pz, Fp1, T5, O1, F7, A2]</td>\n",
       "      <td>300</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.838235</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.171569</td>\n",
       "      <td>11.403351</td>\n",
       "      <td>11.025709</td>\n",
       "      <td>0.377642</td>\n",
       "      <td>14.571427</td>\n",
       "      <td>0.111176</td>\n",
       "      <td>[C3, C4, P4, Cz, Pz, Fp1, Fp2, T3, T5, O2, F7,...</td>\n",
       "      <td>256</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.897059</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.285948</td>\n",
       "      <td>13.947647</td>\n",
       "      <td>14.112779</td>\n",
       "      <td>0.165132</td>\n",
       "      <td>14.325283</td>\n",
       "      <td>0.151270</td>\n",
       "      <td>[P3, F3, F4, C4, Cz, T3, O1]</td>\n",
       "      <td>300</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.359477</td>\n",
       "      <td>20.351732</td>\n",
       "      <td>19.971991</td>\n",
       "      <td>0.379742</td>\n",
       "      <td>25.530256</td>\n",
       "      <td>0.151270</td>\n",
       "      <td>[F3, P4, Fp1, Fp2, T3, O1, F7]</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.212418</td>\n",
       "      <td>11.467754</td>\n",
       "      <td>11.266455</td>\n",
       "      <td>0.201300</td>\n",
       "      <td>11.727347</td>\n",
       "      <td>0.151280</td>\n",
       "      <td>[F3, P4, Fp2, T5, O1, F7, F8, A2, T6]</td>\n",
       "      <td>300</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>13.404001</td>\n",
       "      <td>13.837625</td>\n",
       "      <td>0.433623</td>\n",
       "      <td>17.313883</td>\n",
       "      <td>0.151280</td>\n",
       "      <td>[P3, C3, Fz, C4, P4, Cz, Pz, O1, O2]</td>\n",
       "      <td>300</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>46.463371</td>\n",
       "      <td>47.907314</td>\n",
       "      <td>1.443943</td>\n",
       "      <td>56.868534</td>\n",
       "      <td>0.151295</td>\n",
       "      <td>[P3, C3, F4, C4, P4, Cz, Pz, Fp1, Fp2, T3, F8,...</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.415033</td>\n",
       "      <td>7.995548</td>\n",
       "      <td>8.372165</td>\n",
       "      <td>0.376616</td>\n",
       "      <td>10.104738</td>\n",
       "      <td>0.197561</td>\n",
       "      <td>[F3, P4, Pz, Fp2, O1, T4]</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.985294</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.429739</td>\n",
       "      <td>63.687012</td>\n",
       "      <td>60.733181</td>\n",
       "      <td>2.953831</td>\n",
       "      <td>72.514969</td>\n",
       "      <td>0.197566</td>\n",
       "      <td>[P3, Fz, C4, Cz, Pz, F8, T6]</td>\n",
       "      <td>300</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>6.788703</td>\n",
       "      <td>7.167906</td>\n",
       "      <td>0.379202</td>\n",
       "      <td>7.583030</td>\n",
       "      <td>0.197571</td>\n",
       "      <td>[F4, C4, P4, T3, T5, O1, T6, T4]</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.120915</td>\n",
       "      <td>19.907127</td>\n",
       "      <td>18.919214</td>\n",
       "      <td>0.987913</td>\n",
       "      <td>28.952791</td>\n",
       "      <td>0.197571</td>\n",
       "      <td>[C3, F3, Fz, F4, C4, P4, Cz, Fp1]</td>\n",
       "      <td>300</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.297386</td>\n",
       "      <td>19.269386</td>\n",
       "      <td>18.601891</td>\n",
       "      <td>0.667496</td>\n",
       "      <td>21.792143</td>\n",
       "      <td>0.197576</td>\n",
       "      <td>[P3, C3, F3, Fz, P4, Cz, T5, O2, T4]</td>\n",
       "      <td>256</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>53.967484</td>\n",
       "      <td>50.441456</td>\n",
       "      <td>3.526028</td>\n",
       "      <td>78.822937</td>\n",
       "      <td>0.197581</td>\n",
       "      <td>[C3, F3, Fz, Pz, O1, O2, F7, F8, A2, T4]</td>\n",
       "      <td>300</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.955882</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.400327</td>\n",
       "      <td>4.605590</td>\n",
       "      <td>5.121979</td>\n",
       "      <td>0.516389</td>\n",
       "      <td>11.986703</td>\n",
       "      <td>0.197586</td>\n",
       "      <td>[P3, C3, F4, C4, P4, Pz, Fp2, T5, O1, A2, T6]</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.150327</td>\n",
       "      <td>6.175386</td>\n",
       "      <td>6.102522</td>\n",
       "      <td>0.072864</td>\n",
       "      <td>6.862591</td>\n",
       "      <td>0.197591</td>\n",
       "      <td>[P3, F4, C4, P4, Cz, Pz, Fp1, T3, T5, F8, T6, T4]</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.751465</td>\n",
       "      <td>6.059027</td>\n",
       "      <td>0.307562</td>\n",
       "      <td>6.940506</td>\n",
       "      <td>0.250055</td>\n",
       "      <td>[C3, F3, Fz, C4, P4, Cz, Fp2, O2, A2, T6, T4]</td>\n",
       "      <td>300</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.661765</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.060458</td>\n",
       "      <td>6.682964</td>\n",
       "      <td>6.107259</td>\n",
       "      <td>0.575705</td>\n",
       "      <td>15.494282</td>\n",
       "      <td>0.327206</td>\n",
       "      <td>[P3, C3, P4, Cz, Pz, Fp1, O2, F7, T4]</td>\n",
       "      <td>300</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.163399</td>\n",
       "      <td>7.366287</td>\n",
       "      <td>7.156700</td>\n",
       "      <td>0.209587</td>\n",
       "      <td>7.878020</td>\n",
       "      <td>0.327216</td>\n",
       "      <td>[F3, C4, Pz, Fp2, T3, T5, O2, F8, A2, T6, T4]</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.985294</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.263072</td>\n",
       "      <td>42.407764</td>\n",
       "      <td>40.079468</td>\n",
       "      <td>2.328297</td>\n",
       "      <td>74.947647</td>\n",
       "      <td>0.327221</td>\n",
       "      <td>[P3, C3, F3, Fz, P4, Cz, Pz, Fp2, O1, F8, A2, T6]</td>\n",
       "      <td>128</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.130719</td>\n",
       "      <td>4.013232</td>\n",
       "      <td>4.150839</td>\n",
       "      <td>0.137607</td>\n",
       "      <td>4.764237</td>\n",
       "      <td>0.327221</td>\n",
       "      <td>[P3, C3, F3, Fz, Fp1, Fp2, T3, O1, O2, F8, T6,...</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>48.575909</td>\n",
       "      <td>47.620060</td>\n",
       "      <td>0.955849</td>\n",
       "      <td>66.996628</td>\n",
       "      <td>0.447561</td>\n",
       "      <td>[C3, F3, F4, C4, Pz, T5]</td>\n",
       "      <td>300</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>10.443245</td>\n",
       "      <td>10.687334</td>\n",
       "      <td>0.244089</td>\n",
       "      <td>15.995991</td>\n",
       "      <td>0.447576</td>\n",
       "      <td>[C3, F3, P4, Cz, O1, O2, F7, F8, A2]</td>\n",
       "      <td>300</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.032680</td>\n",
       "      <td>7.132801</td>\n",
       "      <td>6.513338</td>\n",
       "      <td>0.619462</td>\n",
       "      <td>12.093057</td>\n",
       "      <td>0.447591</td>\n",
       "      <td>[P3, C3, F3, Fz, C4, P4, Pz, Fp1, T5, O1, O2, F7]</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.808824</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.253268</td>\n",
       "      <td>16.551935</td>\n",
       "      <td>16.191406</td>\n",
       "      <td>0.360529</td>\n",
       "      <td>19.967964</td>\n",
       "      <td>0.447591</td>\n",
       "      <td>[P3, C3, F3, Cz, Fp2, T3, T5, O1, O2, F8, A2, T4]</td>\n",
       "      <td>256</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train_acc  test_acc   val_acc  train_val_acc_diff  train_loss   val_loss  \\\n",
       "3    0.897059  0.681818  0.777778            0.119281   21.552662  20.884737   \n",
       "18   0.779412  0.454545  0.722222            0.057190   17.909790  16.320948   \n",
       "15   0.779412  0.454545  0.666667            0.112745   12.912975  12.636445   \n",
       "12   1.000000  0.454545  0.666667            0.333333    3.799840   4.123840   \n",
       "11   1.000000  0.636364  0.666667            0.333333   13.156047  12.944667   \n",
       "14   1.000000  0.636364  0.666667            0.333333   50.000744  46.522675   \n",
       "10   0.838235  0.545455  0.666667            0.171569   11.403351  11.025709   \n",
       "2    0.897059  0.681818  0.611111            0.285948   13.947647  14.112779   \n",
       "17   0.970588  0.545455  0.611111            0.359477   20.351732  19.971991   \n",
       "7    0.823529  0.318182  0.611111            0.212418   11.467754  11.266455   \n",
       "9    1.000000  0.454545  0.611111            0.388889   13.404001  13.837625   \n",
       "25   1.000000  0.545455  0.611111            0.388889   46.463371  47.907314   \n",
       "20   0.970588  0.500000  0.555556            0.415033    7.995548   8.372165   \n",
       "8    0.985294  0.545455  0.555556            0.429739   63.687012  60.733181   \n",
       "4    1.000000  0.590909  0.555556            0.444444    6.788703   7.167906   \n",
       "24   0.676471  0.409091  0.555556            0.120915   19.907127  18.919214   \n",
       "6    0.852941  0.545455  0.555556            0.297386   19.269386  18.601891   \n",
       "27   1.000000  0.545455  0.555556            0.444444   53.967484  50.441456   \n",
       "26   0.955882  0.500000  0.555556            0.400327    4.605590   5.121979   \n",
       "1    0.705882  0.545455  0.555556            0.150327    6.175386   6.102522   \n",
       "21   1.000000  0.545455  0.500000            0.500000    5.751465   6.059027   \n",
       "19   0.661765  0.500000  0.722222            0.060458    6.682964   6.107259   \n",
       "0    0.558824  0.590909  0.722222            0.163399    7.366287   7.156700   \n",
       "23   0.985294  0.409091  0.722222            0.263072   42.407764  40.079468   \n",
       "22   0.852941  0.409091  0.722222            0.130719    4.013232   4.150839   \n",
       "16   1.000000  0.500000  0.555556            0.444444   48.575909  47.620060   \n",
       "5    1.000000  0.681818  0.555556            0.444444   10.443245  10.687334   \n",
       "13   0.588235  0.545455  0.555556            0.032680    7.132801   6.513338   \n",
       "28   0.808824  0.727273  0.555556            0.253268   16.551935  16.191406   \n",
       "\n",
       "    train_val_loss_diff  test_loss    scores  \\\n",
       "3              0.667925  24.046930  0.049428   \n",
       "18             1.588842  16.539864  0.077211   \n",
       "15             0.276530  12.697705  0.111136   \n",
       "12             0.324001   8.590641  0.111161   \n",
       "11             0.211380  13.696197  0.111161   \n",
       "14             3.478069  64.703026  0.111166   \n",
       "10             0.377642  14.571427  0.111176   \n",
       "2              0.165132  14.325283  0.151270   \n",
       "17             0.379742  25.530256  0.151270   \n",
       "7              0.201300  11.727347  0.151280   \n",
       "9              0.433623  17.313883  0.151280   \n",
       "25             1.443943  56.868534  0.151295   \n",
       "20             0.376616  10.104738  0.197561   \n",
       "8              2.953831  72.514969  0.197566   \n",
       "4              0.379202   7.583030  0.197571   \n",
       "24             0.987913  28.952791  0.197571   \n",
       "6              0.667496  21.792143  0.197576   \n",
       "27             3.526028  78.822937  0.197581   \n",
       "26             0.516389  11.986703  0.197586   \n",
       "1              0.072864   6.862591  0.197591   \n",
       "21             0.307562   6.940506  0.250055   \n",
       "19             0.575705  15.494282  0.327206   \n",
       "0              0.209587   7.878020  0.327216   \n",
       "23             2.328297  74.947647  0.327221   \n",
       "22             0.137607   4.764237  0.327221   \n",
       "16             0.955849  66.996628  0.447561   \n",
       "5              0.244089  15.995991  0.447576   \n",
       "13             0.619462  12.093057  0.447591   \n",
       "28             0.360529  19.967964  0.447591   \n",
       "\n",
       "                                    channels_selected  sfreq  batch_size  \n",
       "3              [C3, F4, Cz, Fp1, Fp2, T5, F7, A2, T6]    256         128  \n",
       "18         [P4, Fp1, Fp2, T5, O2, F7, F8, A2, T6, T4]    256          32  \n",
       "15                               [P3, Fz, Cz, O2, F8]    256         160  \n",
       "12          [P3, F3, F4, C4, Cz, Fp1, O1, F7, F8, A2]    300          32  \n",
       "11          [C3, F4, C4, Pz, Fp1, T5, F7, A2, T6, T4]    300         224  \n",
       "14      [P3, C3, F4, P4, Cz, Pz, Fp1, T5, O1, F7, A2]    300         160  \n",
       "10  [C3, C4, P4, Cz, Pz, Fp1, Fp2, T3, T5, O2, F7,...    256         192  \n",
       "2                        [P3, F3, F4, C4, Cz, T3, O1]    300         256  \n",
       "17                     [F3, P4, Fp1, Fp2, T3, O1, F7]    256         128  \n",
       "7               [F3, P4, Fp2, T5, O1, F7, F8, A2, T6]    300         224  \n",
       "9                [P3, C3, Fz, C4, P4, Cz, Pz, O1, O2]    300         224  \n",
       "25  [P3, C3, F4, C4, P4, Cz, Pz, Fp1, Fp2, T3, F8,...    256         128  \n",
       "20                          [F3, P4, Pz, Fp2, O1, T4]    256         256  \n",
       "8                        [P3, Fz, C4, Cz, Pz, F8, T6]    300         224  \n",
       "4                    [F4, C4, P4, T3, T5, O1, T6, T4]    256         128  \n",
       "24                  [C3, F3, Fz, F4, C4, P4, Cz, Fp1]    300         224  \n",
       "6                [P3, C3, F3, Fz, P4, Cz, T5, O2, T4]    256         192  \n",
       "27           [C3, F3, Fz, Pz, O1, O2, F7, F8, A2, T4]    300         256  \n",
       "26      [P3, C3, F4, C4, P4, Pz, Fp2, T5, O1, A2, T6]    128          32  \n",
       "1   [P3, F4, C4, P4, Cz, Pz, Fp1, T3, T5, F8, T6, T4]    256         256  \n",
       "21      [C3, F3, Fz, C4, P4, Cz, Fp2, O2, A2, T6, T4]    300         224  \n",
       "19              [P3, C3, P4, Cz, Pz, Fp1, O2, F7, T4]    300          32  \n",
       "0       [F3, C4, Pz, Fp2, T3, T5, O2, F8, A2, T6, T4]    128         128  \n",
       "23  [P3, C3, F3, Fz, P4, Cz, Pz, Fp2, O1, F8, A2, T6]    128         224  \n",
       "22  [P3, C3, F3, Fz, Fp1, Fp2, T3, O1, O2, F8, T6,...    128         128  \n",
       "16                           [C3, F3, F4, C4, Pz, T5]    300         256  \n",
       "5                [C3, F3, P4, Cz, O1, O2, F7, F8, A2]    300          96  \n",
       "13  [P3, C3, F3, Fz, C4, P4, Pz, Fp1, T5, O1, O2, F7]    128          64  \n",
       "28  [P3, C3, F3, Cz, Fp2, T3, T5, O1, O2, F8, A2, T4]    256          96  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_optimizer.get_study_metrics(study).sort_values(by=\"scores\", ascending=True)\n",
    "# model_optimizer.get_study_metrics(study).sort_values(by=\"test_acc\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'sfreq'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">224</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'pool_size_d2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">70</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'strides_d2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'conv_filters_d2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">36</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'conv2d_1_units'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'conv2d_2_units'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'l2_reg_1'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7815602474860956</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'l2_reg_2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04722743699804855</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'l2_reg_3'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6024958313792315</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'dropout_rate'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7000000000000001</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'sfreq'\u001b[0m: \u001b[1;36m256\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'batch_size'\u001b[0m: \u001b[1;36m224\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'pool_size_d2'\u001b[0m: \u001b[1;36m70\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'strides_d2'\u001b[0m: \u001b[1;36m27\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'conv_filters_d2'\u001b[0m: \u001b[1;36m36\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'conv2d_1_units'\u001b[0m: \u001b[1;36m80\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'conv2d_2_units'\u001b[0m: \u001b[1;36m50\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'l2_reg_1'\u001b[0m: \u001b[1;36m0.7815602474860956\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'l2_reg_2'\u001b[0m: \u001b[1;36m0.04722743699804855\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'l2_reg_3'\u001b[0m: \u001b[1;36m0.6024958313792315\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'dropout_rate'\u001b[0m: \u001b[1;36m0.7000000000000001\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">test_accuracy = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.40909090638160706</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "test_accuracy = \u001b[1;36m0.40909090638160706\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">val_accuracy = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7222222089767456</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "val_accuracy = \u001b[1;36m0.7222222089767456\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">channels_selected = <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'P3'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'C3'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'F3'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'Fz'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'F4'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'Cz'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'Fp1'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'T3'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'T5'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'O1'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'O2'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'F8'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'T6'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'T4'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "channels_selected = \u001b[1m[\u001b[0m\u001b[32m'P3'\u001b[0m \u001b[32m'C3'\u001b[0m \u001b[32m'F3'\u001b[0m \u001b[32m'Fz'\u001b[0m \u001b[32m'F4'\u001b[0m \u001b[32m'Cz'\u001b[0m \u001b[32m'Fp1'\u001b[0m \u001b[32m'T3'\u001b[0m \u001b[32m'T5'\u001b[0m \u001b[32m'O1'\u001b[0m \u001b[32m'O2'\u001b[0m \u001b[32m'F8'\u001b[0m \u001b[32m'T6'\u001b[0m \u001b[32m'T4'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rpprint({ k: v for k, v in study.best_trial.params.items() if not k.startswith(\"channels\") })\n",
    "rprint(\"test_accuracy =\", study.best_trial.user_attrs[\"trial_data\"][\"test_accuracy\"])\n",
    "rprint(\"val_accuracy =\", np.max(study.best_trial.user_attrs[\"trial_data\"][\"val_accuracy\"]))\n",
    "rprint(\"channels_selected =\", study.best_trial.user_attrs[\"trial_data\"][\"channels_selected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'./temp/[1]/ce658bbe25c84a238774fa39585c1914/model/study_best_trial.npy'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'./temp/\u001b[0m\u001b[32m[\u001b[0m\u001b[32m1\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/ce658bbe25c84a238774fa39585c1914/model/study_best_trial.npy'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Found <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> best trials for subjects: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Found \u001b[1;36m1\u001b[0m best trials for subjects: \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_optimizer.clean_up(\n",
    "    action=\"remove_all_but_best_trial_data\",\n",
    "    subjects=[1],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moabb_model_optimization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
