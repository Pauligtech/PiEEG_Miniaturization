{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 12:47:24.395031: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-11 12:47:24.395087: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-11 12:47:24.423098: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-11 12:47:24.502270: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-11 12:47:25.483677: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.pick_types is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.pick_channels_regexp is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.channel_type is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use the get_shape_from_baseconcar, InputShapeSetterEEG, BraindecodeDatasetLoaderyou need to install `braindecode`.`pip install braindecode` or Please refer to `https://braindecode.org`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/araz/anaconda3/lib/python3.11/site-packages/moabb/pipelines/__init__.py:26: ModuleNotFoundError: Tensorflow is not installed. You won't be able to use these MOABB pipelines if you attempt to do so.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# region General Imports\n",
    "import os\n",
    "import uuid\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import glob\n",
    "import pickle\n",
    "import tqdm\n",
    "import copy\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mne\n",
    "from rich import print as rprint\n",
    "from rich.pretty import pprint as rpprint\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "# endregion General Imports\n",
    "\n",
    "from model_optim.model_optimizer import ModelOptimizer\n",
    "\n",
    "# Dataset\n",
    "from custom_datasets.fatigue_mi import FatigueMI\n",
    "from custom_datasets.norm_cho import NormCho2017\n",
    "from custom_datasets.opt_game_mi import OptGameMI\n",
    "from custom_datasets.opt_std_mi import OptStdMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_LIST = [\n",
    "    # \"shallow_conv_net\",\n",
    "    \"lstm_net\",\n",
    "    \"deep_conv_net\",\n",
    "    \"eeg_net\",\n",
    "    \"lstm_cnn_net\",\n",
    "    \"lstm_cnn_net_v2\"\n",
    "]\n",
    "MODELS_HYPERPARAMS_DICT = {\n",
    "    \"shallow_conv_net\": {\n",
    "        \"max_epochs\": 5\n",
    "    },\n",
    "    \"eeg_net\": {\n",
    "        \"max_epochs\": 10\n",
    "    },\n",
    "    \"deep_conv_net\": {\n",
    "        \"max_epochs\": 10\n",
    "    },\n",
    "    \"lstm_net\": {\n",
    "        \"max_epochs\": 10\n",
    "    },\n",
    "    \"lstm_cnn_net\": {\n",
    "        \"max_epochs\": 10\n",
    "    },\n",
    "    \"lstm_cnn_net_v2\": {},\n",
    "}\n",
    "\n",
    "DATASETS_LIST = [\n",
    "    FatigueMI,\n",
    "    # NormCho2017,\n",
    "    # OptGameMI,\n",
    "    # OptStdMI,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model_optim.utils import data_generator\n",
    "\n",
    "# data_generator(\n",
    "#     dataset=NormCho2017(),\n",
    "#     subjects=[1],\n",
    "#     channel_idx=[],\n",
    "#     sfreq=128,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_files = glob.glob(\"./temp_v2/*/*/model/study_best_trial.npy\")\n",
    "subject_files_data = {}\n",
    "for subject_file in subject_files:\n",
    "    subject_files_data[subject_file] = np.load(subject_file, allow_pickle=True).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">.<span style=\"color: #800080; text-decoration-color: #800080\">/temp_v2/</span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"font-weight: bold\">]</span><span style=\"color: #800080; text-decoration-color: #800080\">/39d01251ff494106bf04f8a2cffcdd74/model/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">study_best_trial.npy</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       ".\u001b[35m/temp_v2/\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1m]\u001b[0m\u001b[35m/39d01251ff494106bf04f8a2cffcdd74/model/\u001b[0m\u001b[95mstudy_best_trial.npy\u001b[0m \u001b[1;36m0.5\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">.<span style=\"color: #800080; text-decoration-color: #800080\">/temp_v2/</span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span><span style=\"font-weight: bold\">]</span><span style=\"color: #800080; text-decoration-color: #800080\">/a3304348c7094d02a024828ede942cda/model/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">study_best_trial.npy</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5909091234207153</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       ".\u001b[35m/temp_v2/\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1m]\u001b[0m\u001b[35m/a3304348c7094d02a024828ede942cda/model/\u001b[0m\u001b[95mstudy_best_trial.npy\u001b[0m \u001b[1;36m0.5909091234207153\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">.<span style=\"color: #800080; text-decoration-color: #800080\">/temp_v2/</span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">]</span><span style=\"color: #800080; text-decoration-color: #800080\">/d199c9c2ac924b238693f158eb88f675/model/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">study_best_trial.npy</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6363636255264282</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       ".\u001b[35m/temp_v2/\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1m]\u001b[0m\u001b[35m/d199c9c2ac924b238693f158eb88f675/model/\u001b[0m\u001b[95mstudy_best_trial.npy\u001b[0m \u001b[1;36m0.6363636255264282\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">.<span style=\"color: #800080; text-decoration-color: #800080\">/temp_v2/</span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span><span style=\"font-weight: bold\">]</span><span style=\"color: #800080; text-decoration-color: #800080\">/1b189965ada44ff99e73fa145cd3901d/model/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">study_best_trial.npy</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6363636255264282</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       ".\u001b[35m/temp_v2/\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1m]\u001b[0m\u001b[35m/1b189965ada44ff99e73fa145cd3901d/model/\u001b[0m\u001b[95mstudy_best_trial.npy\u001b[0m \u001b[1;36m0.6363636255264282\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">.<span style=\"color: #800080; text-decoration-color: #800080\">/temp_v2/</span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">]</span><span style=\"color: #800080; text-decoration-color: #800080\">/3623cb4ba1ad4a908c9098f5297a6778/model/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">study_best_trial.npy</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8636363744735718</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       ".\u001b[35m/temp_v2/\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[35m/3623cb4ba1ad4a908c9098f5297a6778/model/\u001b[0m\u001b[95mstudy_best_trial.npy\u001b[0m \u001b[1;36m0.8636363744735718\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">.<span style=\"color: #800080; text-decoration-color: #800080\">/temp_v2/</span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">]</span><span style=\"color: #800080; text-decoration-color: #800080\">/9c1b753483db409a90eab7b7149b8af8/model/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">study_best_trial.npy</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6818181872367859</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       ".\u001b[35m/temp_v2/\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m]\u001b[0m\u001b[35m/9c1b753483db409a90eab7b7149b8af8/model/\u001b[0m\u001b[95mstudy_best_trial.npy\u001b[0m \u001b[1;36m0.6818181872367859\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">.<span style=\"color: #800080; text-decoration-color: #800080\">/temp_v2/</span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"font-weight: bold\">]</span><span style=\"color: #800080; text-decoration-color: #800080\">/63a8c87ffc02471893db5ac9a0781946/model/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">study_best_trial.npy</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9090909361839294</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       ".\u001b[35m/temp_v2/\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1m]\u001b[0m\u001b[35m/63a8c87ffc02471893db5ac9a0781946/model/\u001b[0m\u001b[95mstudy_best_trial.npy\u001b[0m \u001b[1;36m0.9090909361839294\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">.<span style=\"color: #800080; text-decoration-color: #800080\">/temp_v2/</span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span><span style=\"color: #800080; text-decoration-color: #800080\">/292cbc92b8cf46da9986fe7d8447819f/model/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">study_best_trial.npy</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8636363744735718</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       ".\u001b[35m/temp_v2/\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[35m/292cbc92b8cf46da9986fe7d8447819f/model/\u001b[0m\u001b[95mstudy_best_trial.npy\u001b[0m \u001b[1;36m0.8636363744735718\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">.<span style=\"color: #800080; text-decoration-color: #800080\">/temp_v2/</span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"font-weight: bold\">]</span><span style=\"color: #800080; text-decoration-color: #800080\">/aabe056cd1954a6f92ab47d84c86b1b8/model/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">study_best_trial.npy</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7272727489471436</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       ".\u001b[35m/temp_v2/\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1m]\u001b[0m\u001b[35m/aabe056cd1954a6f92ab47d84c86b1b8/model/\u001b[0m\u001b[95mstudy_best_trial.npy\u001b[0m \u001b[1;36m0.7272727489471436\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">.<span style=\"color: #800080; text-decoration-color: #800080\">/temp_v2/</span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]</span><span style=\"color: #800080; text-decoration-color: #800080\">/6405d11e654b42aca9df48458c67ecde/model/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">study_best_trial.npy</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6363636255264282</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       ".\u001b[35m/temp_v2/\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[35m/6405d11e654b42aca9df48458c67ecde/model/\u001b[0m\u001b[95mstudy_best_trial.npy\u001b[0m \u001b[1;36m0.6363636255264282\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">.<span style=\"color: #800080; text-decoration-color: #800080\">/temp_v2/</span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">]</span><span style=\"color: #800080; text-decoration-color: #800080\">/9fd82ec44ef3496da6307b57ecf4532f/model/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">study_best_trial.npy</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6363636255264282</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       ".\u001b[35m/temp_v2/\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1m]\u001b[0m\u001b[35m/9fd82ec44ef3496da6307b57ecf4532f/model/\u001b[0m\u001b[95mstudy_best_trial.npy\u001b[0m \u001b[1;36m0.6363636255264282\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">.<span style=\"color: #800080; text-decoration-color: #800080\">/temp_v2/</span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"font-weight: bold\">]</span><span style=\"color: #800080; text-decoration-color: #800080\">/c2cc69dca74d4bfa81722cd634e6403e/model/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">study_best_trial.npy</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7727273106575012</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       ".\u001b[35m/temp_v2/\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1m]\u001b[0m\u001b[35m/c2cc69dca74d4bfa81722cd634e6403e/model/\u001b[0m\u001b[95mstudy_best_trial.npy\u001b[0m \u001b[1;36m0.7727273106575012\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">.<span style=\"color: #800080; text-decoration-color: #800080\">/temp_v2/</span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"font-weight: bold\">]</span><span style=\"color: #800080; text-decoration-color: #800080\">/96dc576945fb4f2db582d66ae1d2c8ce/model/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">study_best_trial.npy</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7727273106575012</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       ".\u001b[35m/temp_v2/\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1m]\u001b[0m\u001b[35m/96dc576945fb4f2db582d66ae1d2c8ce/model/\u001b[0m\u001b[95mstudy_best_trial.npy\u001b[0m \u001b[1;36m0.7727273106575012\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">.<span style=\"color: #800080; text-decoration-color: #800080\">/temp_v2/</span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span><span style=\"font-weight: bold\">]</span><span style=\"color: #800080; text-decoration-color: #800080\">/e0643f9a780146a4adc15ddd4a9ff053/model/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">study_best_trial.npy</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7272727489471436</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       ".\u001b[35m/temp_v2/\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1m]\u001b[0m\u001b[35m/e0643f9a780146a4adc15ddd4a9ff053/model/\u001b[0m\u001b[95mstudy_best_trial.npy\u001b[0m \u001b[1;36m0.7272727489471436\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for subject_file_data_key, subject_file_data in subject_files_data.items():\n",
    "    rprint(subject_file_data_key, subject_file_data.user_attrs['trial_data']['test_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sfreq: 300.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a822c841c6224976b79eafd5356f295d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding metadata with 3 columns\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 12:47:30.275391: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:13:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-11 12:47:30.414748: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:13:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-11 12:47:30.414828: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:13:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-11 12:47:30.416426: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:13:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-11 12:47:30.416496: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:13:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-11 12:47:30.416543: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:13:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-11 12:47:31.628390: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:13:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-11 12:47:31.628473: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:13:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-11 12:47:31.628480: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-04-11 12:47:31.628534: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:13:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-11 12:47:31.628797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9537 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:13:00.0, compute capability: 8.6\n",
      "2024-04-11 12:47:32.920669: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2024-04-11 12:47:34.631894: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2024-04-11 12:47:35.839861: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-04-11 12:47:36.183464: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-04-11 12:47:36.639072: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f1d7397a7f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-11 12:47:36.639099: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2024-04-11 12:47:36.643293: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1712861256.674262  127086 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 4.2829 - accuracy: 0.3971\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.50000, storing weights.\n",
      "2/2 [==============================] - 5s 410ms/step - loss: 4.2829 - accuracy: 0.3971 - val_loss: 1.3055 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.2678 - accuracy: 0.6406\n",
      "Epoch 00002: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.2675 - accuracy: 0.6324 - val_loss: 1.2439 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.1978 - accuracy: 0.5625\n",
      "Epoch 00003: val_accuracy improved from 0.50000 to 0.55556, storing weights.\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.1984 - accuracy: 0.5588 - val_loss: 1.1956 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.1316 - accuracy: 0.7344\n",
      "Epoch 00004: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.1317 - accuracy: 0.7206 - val_loss: 1.1562 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.0892 - accuracy: 0.6875\n",
      "Epoch 00005: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.0902 - accuracy: 0.6912 - val_loss: 1.1217 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.0307 - accuracy: 0.8125\n",
      "Epoch 00006: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.0327 - accuracy: 0.7941 - val_loss: 1.0916 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.9889 - accuracy: 0.7500\n",
      "Epoch 00007: val_accuracy improved from 0.55556 to 0.61111, storing weights.\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.9949 - accuracy: 0.7353 - val_loss: 1.0653 - val_accuracy: 0.6111 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.9505 - accuracy: 0.7656\n",
      "Epoch 00008: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.9460 - accuracy: 0.7794 - val_loss: 1.0436 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.9186 - accuracy: 0.7188\n",
      "Epoch 00009: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.9143 - accuracy: 0.7206 - val_loss: 1.0229 - val_accuracy: 0.6111 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.8759 - accuracy: 0.7656\n",
      "Epoch 00010: val_accuracy improved from 0.61111 to 0.66667, storing weights.\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.8684 - accuracy: 0.7794 - val_loss: 1.0051 - val_accuracy: 0.6667 - lr: 0.0010\n",
      "Using epoch 00010 with val_accuracy: 0.66667\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.0055 - accuracy: 0.5455\n",
      "\n",
      "\n",
      "Adding metadata with 3 columns\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.6550 - accuracy: 0.5882\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.55556, storing weights.\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.6550 - accuracy: 0.5882 - val_loss: 1.7229 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7098 - accuracy: 0.6176\n",
      "Epoch 00002: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.7098 - accuracy: 0.6176 - val_loss: 1.5703 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5449 - accuracy: 0.7059\n",
      "Epoch 00003: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.5449 - accuracy: 0.7059 - val_loss: 1.4617 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4285 - accuracy: 0.7500\n",
      "Epoch 00004: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.4285 - accuracy: 0.7500 - val_loss: 1.3764 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3470 - accuracy: 0.6471\n",
      "Epoch 00005: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.3470 - accuracy: 0.6471 - val_loss: 1.3062 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2538 - accuracy: 0.7794\n",
      "Epoch 00006: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.2538 - accuracy: 0.7794 - val_loss: 1.2475 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1844 - accuracy: 0.8088\n",
      "Epoch 00007: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1844 - accuracy: 0.8088 - val_loss: 1.1968 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1230 - accuracy: 0.7647\n",
      "Epoch 00008: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.1230 - accuracy: 0.7647 - val_loss: 1.1531 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0647 - accuracy: 0.7647\n",
      "Epoch 00009: val_accuracy improved from 0.55556 to 0.61111, storing weights.\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0647 - accuracy: 0.7647 - val_loss: 1.1141 - val_accuracy: 0.6111 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0090 - accuracy: 0.7206\n",
      "Epoch 00010: val_accuracy improved from 0.61111 to 0.66667, storing weights.\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.0090 - accuracy: 0.7206 - val_loss: 1.0791 - val_accuracy: 0.6667 - lr: 0.0010\n",
      "Using epoch 00010 with val_accuracy: 0.66667\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.0788 - accuracy: 0.8182\n",
      "\n",
      "\n",
      "Adding metadata with 3 columns\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.0815 - accuracy: 0.5294\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.44444, storing weights.\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.0815 - accuracy: 0.5294 - val_loss: 1.9602 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9417 - accuracy: 0.6471\n",
      "Epoch 00002: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.9417 - accuracy: 0.6471 - val_loss: 1.7582 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7433 - accuracy: 0.6618\n",
      "Epoch 00003: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.7433 - accuracy: 0.6618 - val_loss: 1.6163 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5871 - accuracy: 0.6912\n",
      "Epoch 00004: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.5871 - accuracy: 0.6912 - val_loss: 1.5055 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4656 - accuracy: 0.6765\n",
      "Epoch 00005: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.4656 - accuracy: 0.6765 - val_loss: 1.4153 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3778 - accuracy: 0.6765\n",
      "Epoch 00006: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.3778 - accuracy: 0.6765 - val_loss: 1.3385 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2913 - accuracy: 0.7059\n",
      "Epoch 00007: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.2913 - accuracy: 0.7059 - val_loss: 1.2730 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2259 - accuracy: 0.6618\n",
      "Epoch 00008: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.2259 - accuracy: 0.6618 - val_loss: 1.2158 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1590 - accuracy: 0.7206\n",
      "Epoch 00009: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.1590 - accuracy: 0.7206 - val_loss: 1.1658 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1049 - accuracy: 0.6765\n",
      "Epoch 00010: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.1049 - accuracy: 0.6765 - val_loss: 1.1211 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Using epoch 00001 with val_accuracy: 0.44444\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.9600 - accuracy: 0.5000\n",
      "\n",
      "\n",
      "Adding metadata with 3 columns\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.8610 - accuracy: 0.4412\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.55556, storing weights.\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.8610 - accuracy: 0.4412 - val_loss: 1.8947 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.8730 - accuracy: 0.5441\n",
      "Epoch 00002: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.8730 - accuracy: 0.5441 - val_loss: 1.7311 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7084 - accuracy: 0.5882\n",
      "Epoch 00003: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.7084 - accuracy: 0.5882 - val_loss: 1.6129 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5726 - accuracy: 0.6324\n",
      "Epoch 00004: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.5726 - accuracy: 0.6324 - val_loss: 1.5195 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4446 - accuracy: 0.7353\n",
      "Epoch 00005: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.4446 - accuracy: 0.7353 - val_loss: 1.4402 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3660 - accuracy: 0.7059\n",
      "Epoch 00006: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.3660 - accuracy: 0.7059 - val_loss: 1.3729 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2905 - accuracy: 0.7353\n",
      "Epoch 00007: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.2905 - accuracy: 0.7353 - val_loss: 1.3149 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2189 - accuracy: 0.7647\n",
      "Epoch 00008: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.2189 - accuracy: 0.7647 - val_loss: 1.2643 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1461 - accuracy: 0.7647\n",
      "Epoch 00009: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.1461 - accuracy: 0.7647 - val_loss: 1.2180 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1061 - accuracy: 0.7794\n",
      "Epoch 00010: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.1061 - accuracy: 0.7794 - val_loss: 1.1776 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Using epoch 00001 with val_accuracy: 0.55556\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.8955 - accuracy: 0.5000\n",
      "\n",
      "\n",
      "Adding metadata with 3 columns\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5732 - accuracy: 0.5000\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.55556, storing weights.\n",
      "2/2 [==============================] - 2s 273ms/step - loss: 3.5732 - accuracy: 0.5000 - val_loss: 1.0618 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.0243 - accuracy: 0.6406\n",
      "Epoch 00002: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.0280 - accuracy: 0.6029 - val_loss: 1.0434 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.0081 - accuracy: 0.6875\n",
      "Epoch 00003: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.0049 - accuracy: 0.7059 - val_loss: 1.0278 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.0046 - accuracy: 0.5625\n",
      "Epoch 00004: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.9975 - accuracy: 0.5882 - val_loss: 1.0133 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.0104 - accuracy: 0.6250\n",
      "Epoch 00005: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.0032 - accuracy: 0.6324 - val_loss: 0.9998 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.9958 - accuracy: 0.5156\n",
      "Epoch 00006: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.9888 - accuracy: 0.5441 - val_loss: 0.9878 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.9582 - accuracy: 0.6406\n",
      "Epoch 00007: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9514 - accuracy: 0.6471 - val_loss: 0.9773 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.9476 - accuracy: 0.5938\n",
      "Epoch 00008: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9463 - accuracy: 0.5882 - val_loss: 0.9666 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.9598 - accuracy: 0.5156\n",
      "Epoch 00009: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9645 - accuracy: 0.5147 - val_loss: 0.9572 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.9150 - accuracy: 0.6875\n",
      "Epoch 00010: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9157 - accuracy: 0.6765 - val_loss: 0.9483 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Using epoch 00001 with val_accuracy: 0.55556\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.0631 - accuracy: 0.5000\n",
      "\n",
      "\n",
      "Adding metadata with 3 columns\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7293 - accuracy: 0.4118\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.50000, storing weights.\n",
      "2/2 [==============================] - 1s 242ms/step - loss: 3.7293 - accuracy: 0.4118 - val_loss: 1.8550 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.8028 - accuracy: 0.7656\n",
      "Epoch 00002: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.7951 - accuracy: 0.7647 - val_loss: 1.6036 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.5473 - accuracy: 0.6719\n",
      "Epoch 00003: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.5477 - accuracy: 0.6471 - val_loss: 1.4427 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.3938 - accuracy: 0.6719\n",
      "Epoch 00004: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.3923 - accuracy: 0.6471 - val_loss: 1.3257 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.2733 - accuracy: 0.6875\n",
      "Epoch 00005: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.2725 - accuracy: 0.6765 - val_loss: 1.2358 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.1777 - accuracy: 0.7344\n",
      "Epoch 00006: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.1738 - accuracy: 0.7500 - val_loss: 1.1635 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.0843 - accuracy: 0.7812\n",
      "Epoch 00007: val_accuracy improved from 0.50000 to 0.55556, storing weights.\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.0834 - accuracy: 0.7794 - val_loss: 1.1059 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.0075 - accuracy: 0.7656\n",
      "Epoch 00008: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 1.0023 - accuracy: 0.7794 - val_loss: 1.0577 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.9727 - accuracy: 0.7031\n",
      "Epoch 00009: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.9673 - accuracy: 0.7059 - val_loss: 1.0184 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.8893 - accuracy: 0.6719\n",
      "Epoch 00010: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.8896 - accuracy: 0.6765 - val_loss: 0.9865 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Using epoch 00007 with val_accuracy: 0.55556\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.1058 - accuracy: 0.5455\n",
      "\n",
      "\n",
      "Adding metadata with 3 columns\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2368 - accuracy: 0.3824\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.50000, storing weights.\n",
      "2/2 [==============================] - 1s 250ms/step - loss: 3.2368 - accuracy: 0.3824 - val_loss: 1.3854 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.3713 - accuracy: 0.5781\n",
      "Epoch 00002: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.3681 - accuracy: 0.5735 - val_loss: 1.2785 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.2496 - accuracy: 0.7031\n",
      "Epoch 00003: val_accuracy improved from 0.50000 to 0.55556, storing weights.\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.2461 - accuracy: 0.7059 - val_loss: 1.2023 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.1743 - accuracy: 0.6562\n",
      "Epoch 00004: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.1720 - accuracy: 0.6618 - val_loss: 1.1406 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.1068 - accuracy: 0.6562\n",
      "Epoch 00005: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.1058 - accuracy: 0.6471 - val_loss: 1.0917 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.0323 - accuracy: 0.6875\n",
      "Epoch 00006: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.0315 - accuracy: 0.6912 - val_loss: 1.0499 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.9811 - accuracy: 0.6250\n",
      "Epoch 00007: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.9792 - accuracy: 0.6324 - val_loss: 1.0145 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.9325 - accuracy: 0.6875\n",
      "Epoch 00008: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.9306 - accuracy: 0.7059 - val_loss: 0.9837 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.9022 - accuracy: 0.6562\n",
      "Epoch 00009: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.8939 - accuracy: 0.6618 - val_loss: 0.9591 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.8455 - accuracy: 0.7031\n",
      "Epoch 00010: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.8494 - accuracy: 0.6912 - val_loss: 0.9347 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Using epoch 00003 with val_accuracy: 0.55556\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.2025 - accuracy: 0.5000\n",
      "\n",
      "\n",
      "Adding metadata with 3 columns\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.5468 - accuracy: 0.4706\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.44444, storing weights.\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3.5468 - accuracy: 0.4706 - val_loss: 2.2034 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.1851 - accuracy: 0.5588\n",
      "Epoch 00002: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.1851 - accuracy: 0.5588 - val_loss: 2.0798 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.0489 - accuracy: 0.6324\n",
      "Epoch 00003: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.0489 - accuracy: 0.6324 - val_loss: 1.9843 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9355 - accuracy: 0.7353\n",
      "Epoch 00004: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.9355 - accuracy: 0.7353 - val_loss: 1.9048 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.8505 - accuracy: 0.6765\n",
      "Epoch 00005: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.8505 - accuracy: 0.6765 - val_loss: 1.8360 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7707 - accuracy: 0.7206\n",
      "Epoch 00006: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.7707 - accuracy: 0.7206 - val_loss: 1.7750 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7091 - accuracy: 0.6618\n",
      "Epoch 00007: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.7091 - accuracy: 0.6618 - val_loss: 1.7196 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6399 - accuracy: 0.7059\n",
      "Epoch 00008: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.6399 - accuracy: 0.7059 - val_loss: 1.6691 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5728 - accuracy: 0.7059\n",
      "Epoch 00009: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.5728 - accuracy: 0.7059 - val_loss: 1.6226 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4950 - accuracy: 0.7794\n",
      "Epoch 00010: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.4950 - accuracy: 0.7794 - val_loss: 1.5801 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Using epoch 00001 with val_accuracy: 0.44444\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 2.2031 - accuracy: 0.5000\n",
      "\n",
      "\n",
      "Adding metadata with 3 columns\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7805 - accuracy: 0.5147\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.55556, storing weights.\n",
      "2/2 [==============================] - 1s 269ms/step - loss: 2.7805 - accuracy: 0.5147 - val_loss: 1.8060 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.8107 - accuracy: 0.5781\n",
      "Epoch 00002: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 1.8044 - accuracy: 0.5882 - val_loss: 1.6528 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.6631 - accuracy: 0.4219\n",
      "Epoch 00003: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.6594 - accuracy: 0.4265 - val_loss: 1.5422 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.5566 - accuracy: 0.4688\n",
      "Epoch 00004: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.5519 - accuracy: 0.4706 - val_loss: 1.4526 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.4611 - accuracy: 0.5000\n",
      "Epoch 00005: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.4598 - accuracy: 0.5000 - val_loss: 1.3772 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.3723 - accuracy: 0.5469\n",
      "Epoch 00006: val_accuracy improved from 0.55556 to 0.61111, storing weights.\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.3744 - accuracy: 0.5294 - val_loss: 1.3103 - val_accuracy: 0.6111 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.3090 - accuracy: 0.5469\n",
      "Epoch 00007: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.3090 - accuracy: 0.5294 - val_loss: 1.2527 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.2527 - accuracy: 0.5625\n",
      "Epoch 00008: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.2515 - accuracy: 0.5441 - val_loss: 1.2010 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.1895 - accuracy: 0.6094\n",
      "Epoch 00009: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.1894 - accuracy: 0.5882 - val_loss: 1.1524 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.1472 - accuracy: 0.5156\n",
      "Epoch 00010: val_accuracy did not improve\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.1455 - accuracy: 0.5000 - val_loss: 1.1096 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Using epoch 00006 with val_accuracy: 0.61111\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.3120 - accuracy: 0.5000\n",
      "\n",
      "\n",
      "Adding metadata with 3 columns\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.8678 - accuracy: 0.5882\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.44444, storing weights.\n",
      "3/3 [==============================] - 1s 132ms/step - loss: 1.8678 - accuracy: 0.5882 - val_loss: 1.2943 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2754 - accuracy: 0.5625\n",
      "Epoch 00002: val_accuracy did not improve\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1.2579 - accuracy: 0.5882 - val_loss: 1.2068 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.2515 - accuracy: 0.2500\n",
      "Epoch 00003: val_accuracy did not improve\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 1.2184 - accuracy: 0.3676 - val_loss: 1.1410 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.1347 - accuracy: 0.6562\n",
      "Epoch 00004: val_accuracy did not improve\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1.1278 - accuracy: 0.5882 - val_loss: 1.0867 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0661 - accuracy: 0.6562\n",
      "Epoch 00005: val_accuracy did not improve\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 1.0764 - accuracy: 0.5735 - val_loss: 1.0410 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.0224 - accuracy: 0.5938\n",
      "Epoch 00006: val_accuracy did not improve\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 1.0233 - accuracy: 0.6029 - val_loss: 1.0018 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.9702 - accuracy: 0.6250\n",
      "Epoch 00007: val_accuracy improved from 0.44444 to 0.50000, storing weights.\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.9806 - accuracy: 0.5294 - val_loss: 0.9679 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.9709 - accuracy: 0.5000\n",
      "Epoch 00008: val_accuracy did not improve\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.9607 - accuracy: 0.5147 - val_loss: 0.9385 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.9184 - accuracy: 0.5000\n",
      "Epoch 00009: val_accuracy did not improve\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.9134 - accuracy: 0.5882 - val_loss: 0.9136 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.8894 - accuracy: 0.6875\n",
      "Epoch 00010: val_accuracy did not improve\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.8903 - accuracy: 0.6471 - val_loss: 0.8900 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Using epoch 00007 with val_accuracy: 0.50000\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.9678 - accuracy: 0.5000\n",
      "\n",
      "\n",
      "Adding metadata with 3 columns\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.6793 - accuracy: 0.3971\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.44444, storing weights.\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.6793 - accuracy: 0.3971 - val_loss: 0.8200 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8762 - accuracy: 0.4118\n",
      "Epoch 00002: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.8762 - accuracy: 0.4118 - val_loss: 0.8160 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8623 - accuracy: 0.4853\n",
      "Epoch 00003: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.8623 - accuracy: 0.4853 - val_loss: 0.8120 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8290 - accuracy: 0.4559\n",
      "Epoch 00004: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.8290 - accuracy: 0.4559 - val_loss: 0.8087 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8006 - accuracy: 0.5147\n",
      "Epoch 00005: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.8006 - accuracy: 0.5147 - val_loss: 0.8054 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8099 - accuracy: 0.5000\n",
      "Epoch 00006: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.8099 - accuracy: 0.5000 - val_loss: 0.8019 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8035 - accuracy: 0.6176\n",
      "Epoch 00007: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.8035 - accuracy: 0.6176 - val_loss: 0.7990 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8095 - accuracy: 0.5000\n",
      "Epoch 00008: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.8095 - accuracy: 0.5000 - val_loss: 0.7960 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7908 - accuracy: 0.5294\n",
      "Epoch 00009: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.7908 - accuracy: 0.5294 - val_loss: 0.7931 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7933 - accuracy: 0.5735\n",
      "Epoch 00010: val_accuracy did not improve\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.7933 - accuracy: 0.5735 - val_loss: 0.7904 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Using epoch 00001 with val_accuracy: 0.44444\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.8197 - accuracy: 0.5000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset in [FatigueMI]:\n",
    "    for model in [\"eeg_net\"]:\n",
    "        model_optimizer = ModelOptimizer(\n",
    "            dataset=dataset(),\n",
    "            model_name=model\n",
    "        )\n",
    "        for subject in [12]:\n",
    "            max_epochs = MODELS_HYPERPARAMS_DICT[model][\"max_epochs\"]\n",
    "            study = model_optimizer.search_best_model(\n",
    "                subjects = [subject],\n",
    "                max_iter = 25,\n",
    "                max_epochs = max_epochs,\n",
    "                max_stag_count = 10,\n",
    "                rounds = 1,\n",
    "                replace_previous_study_for_subjects = False\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>train_val_acc_diff</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>train_val_loss_diff</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>scores</th>\n",
       "      <th>channels_selected</th>\n",
       "      <th>sfreq</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.127451</td>\n",
       "      <td>0.868390</td>\n",
       "      <td>1.005088</td>\n",
       "      <td>0.136698</td>\n",
       "      <td>1.005517</td>\n",
       "      <td>0.111561</td>\n",
       "      <td>[F4, C4, P4, Fp1, T3, T5, O1, O2, T4]</td>\n",
       "      <td>128.0</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.779412</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.223856</td>\n",
       "      <td>0.889572</td>\n",
       "      <td>0.986514</td>\n",
       "      <td>0.096942</td>\n",
       "      <td>1.105826</td>\n",
       "      <td>0.198131</td>\n",
       "      <td>[P3, C3, C4, Cz, Pz, Fp1, O1, O2, F7, F8, T6, T4]</td>\n",
       "      <td>128.0</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.808824</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.142157</td>\n",
       "      <td>1.008998</td>\n",
       "      <td>1.079126</td>\n",
       "      <td>0.070128</td>\n",
       "      <td>1.078813</td>\n",
       "      <td>0.861611</td>\n",
       "      <td>[Fz, C4, P4, Cz, Fp2, T3, O2, F7, A2, T4]</td>\n",
       "      <td>128.0</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.150327</td>\n",
       "      <td>0.849370</td>\n",
       "      <td>0.934690</td>\n",
       "      <td>0.085320</td>\n",
       "      <td>1.202529</td>\n",
       "      <td>0.947931</td>\n",
       "      <td>[F3, Fz, F4, Fp2, T5, F8, A2, T4]</td>\n",
       "      <td>128.0</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.890274</td>\n",
       "      <td>0.890007</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.967755</td>\n",
       "      <td>1.000750</td>\n",
       "      <td>[P3, F3, Fz, C4, P4, Cz, Pz, Fp2, T5, O1, O2, ...</td>\n",
       "      <td>128.0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.720588</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.276144</td>\n",
       "      <td>1.104852</td>\n",
       "      <td>1.121118</td>\n",
       "      <td>0.016266</td>\n",
       "      <td>1.960047</td>\n",
       "      <td>1.059242</td>\n",
       "      <td>[P3, C3, F3, Fz, F4, P4, Cz, Pz, T3, O2, F7, A2]</td>\n",
       "      <td>256.0</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.779412</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.334967</td>\n",
       "      <td>1.495029</td>\n",
       "      <td>1.580115</td>\n",
       "      <td>0.085086</td>\n",
       "      <td>2.203085</td>\n",
       "      <td>1.059242</td>\n",
       "      <td>[P3, F3, F4, C4, P4, Cz, Pz, Fp2, T5, O2, T6, T4]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.022876</td>\n",
       "      <td>1.145460</td>\n",
       "      <td>1.109649</td>\n",
       "      <td>0.035811</td>\n",
       "      <td>1.311961</td>\n",
       "      <td>1.151785</td>\n",
       "      <td>[C3, Fz, P4, Pz, Fp2, T3, T5, O1, F7, A2, T4]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.779412</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.223856</td>\n",
       "      <td>1.106127</td>\n",
       "      <td>1.177586</td>\n",
       "      <td>0.071458</td>\n",
       "      <td>1.895464</td>\n",
       "      <td>1.197831</td>\n",
       "      <td>[C3, F3, C4, Cz, Pz, O2]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.150327</td>\n",
       "      <td>0.915680</td>\n",
       "      <td>0.948252</td>\n",
       "      <td>0.032573</td>\n",
       "      <td>1.063092</td>\n",
       "      <td>1.197931</td>\n",
       "      <td>[F3, Fz, Pz, Fp1, Fp2, O1, F7, T6]</td>\n",
       "      <td>256.0</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.173203</td>\n",
       "      <td>0.790843</td>\n",
       "      <td>0.790366</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.819663</td>\n",
       "      <td>1.309042</td>\n",
       "      <td>[F4, C4, P4, Fp1, T3, T5, O1, F8]</td>\n",
       "      <td>128.0</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train_acc  test_acc   val_acc  train_val_acc_diff  train_loss  val_loss  \\\n",
       "0    0.794118  0.545455  0.666667            0.127451    0.868390  1.005088   \n",
       "5    0.779412  0.545455  0.555556            0.223856    0.889572  0.986514   \n",
       "1    0.808824  0.818182  0.666667            0.142157    1.008998  1.079126   \n",
       "6    0.705882  0.500000  0.555556            0.150327    0.849370  0.934690   \n",
       "9    0.647059  0.500000  0.500000            0.147059    0.890274  0.890007   \n",
       "2    0.720588  0.500000  0.444444            0.276144    1.104852  1.121118   \n",
       "7    0.779412  0.500000  0.444444            0.334967    1.495029  1.580115   \n",
       "8    0.588235  0.500000  0.611111            0.022876    1.145460  1.109649   \n",
       "3    0.779412  0.500000  0.555556            0.223856    1.106127  1.177586   \n",
       "4    0.705882  0.500000  0.555556            0.150327    0.915680  0.948252   \n",
       "10   0.617647  0.500000  0.444444            0.173203    0.790843  0.790366   \n",
       "\n",
       "    train_val_loss_diff  test_loss    scores  \\\n",
       "0              0.136698   1.005517  0.111561   \n",
       "5              0.096942   1.105826  0.198131   \n",
       "1              0.070128   1.078813  0.861611   \n",
       "6              0.085320   1.202529  0.947931   \n",
       "9              0.000267   0.967755  1.000750   \n",
       "2              0.016266   1.960047  1.059242   \n",
       "7              0.085086   2.203085  1.059242   \n",
       "8              0.035811   1.311961  1.151785   \n",
       "3              0.071458   1.895464  1.197831   \n",
       "4              0.032573   1.063092  1.197931   \n",
       "10             0.000477   0.819663  1.309042   \n",
       "\n",
       "                                    channels_selected  sfreq  batch_size  \n",
       "0               [F4, C4, P4, Fp1, T3, T5, O1, O2, T4]  128.0          64  \n",
       "5   [P3, C3, C4, Cz, Pz, Fp1, O1, O2, F7, F8, T6, T4]  128.0          64  \n",
       "1           [Fz, C4, P4, Cz, Fp2, T3, O2, F7, A2, T4]  128.0          96  \n",
       "6                   [F3, Fz, F4, Fp2, T5, F8, A2, T4]  128.0          64  \n",
       "9   [P3, F3, Fz, C4, P4, Cz, Pz, Fp2, T5, O1, O2, ...  128.0          32  \n",
       "2    [P3, C3, F3, Fz, F4, P4, Cz, Pz, T3, O2, F7, A2]  256.0         128  \n",
       "7   [P3, F3, F4, C4, P4, Cz, Pz, Fp2, T5, O2, T6, T4]    NaN         160  \n",
       "8       [C3, Fz, P4, Pz, Fp2, T3, T5, O1, F7, A2, T4]    NaN          64  \n",
       "3                            [C3, F3, C4, Cz, Pz, O2]    NaN         160  \n",
       "4                  [F3, Fz, Pz, Fp1, Fp2, O1, F7, T6]  256.0          64  \n",
       "10                  [F4, C4, P4, Fp1, T3, T5, O1, F8]  128.0         256  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_optimizer.get_study_metrics(study).sort_values(by=\"scores\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'sfreq'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'conv2d_1_units'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'conv2d_1_kernl_length'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'pool_1_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'conv2d_depth_multiplier'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'conv2d_2_units'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'conv2d_2_kernl_length'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'pool_2_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'l2_reg_1'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.32825455382862395</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'l2_reg_2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5696184885654687</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'l2_reg_3'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.485802254049597</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'l2_reg_4'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8027068971954714</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'dropout_rate'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'sfreq'\u001b[0m: \u001b[1;36m128\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'batch_size'\u001b[0m: \u001b[1;36m64\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'conv2d_1_units'\u001b[0m: \u001b[1;36m40\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'conv2d_1_kernl_length'\u001b[0m: \u001b[1;36m16\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'pool_1_size'\u001b[0m: \u001b[1;36m8\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'conv2d_depth_multiplier'\u001b[0m: \u001b[1;36m8\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'conv2d_2_units'\u001b[0m: \u001b[1;36m64\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'conv2d_2_kernl_length'\u001b[0m: \u001b[1;36m80\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'pool_2_size'\u001b[0m: \u001b[1;36m8\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'l2_reg_1'\u001b[0m: \u001b[1;36m0.32825455382862395\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'l2_reg_2'\u001b[0m: \u001b[1;36m0.5696184885654687\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'l2_reg_3'\u001b[0m: \u001b[1;36m0.485802254049597\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'l2_reg_4'\u001b[0m: \u001b[1;36m0.8027068971954714\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'dropout_rate'\u001b[0m: \u001b[1;36m0.4\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">test_accuracy = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5454545617103577</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "test_accuracy = \u001b[1;36m0.5454545617103577\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">val_accuracy = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6666666865348816</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "val_accuracy = \u001b[1;36m0.6666666865348816\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">channels_selected = <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'F4'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'C4'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'P4'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'Fp1'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'T3'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'T5'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'O1'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'O2'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'T4'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "channels_selected = \u001b[1m[\u001b[0m\u001b[32m'F4'\u001b[0m \u001b[32m'C4'\u001b[0m \u001b[32m'P4'\u001b[0m \u001b[32m'Fp1'\u001b[0m \u001b[32m'T3'\u001b[0m \u001b[32m'T5'\u001b[0m \u001b[32m'O1'\u001b[0m \u001b[32m'O2'\u001b[0m \u001b[32m'T4'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rpprint({ k: v for k, v in study.best_trial.params.items() if not k.startswith(\"channels\") })\n",
    "rprint(\"test_accuracy =\", study.best_trial.user_attrs[\"trial_data\"][\"test_accuracy\"])\n",
    "rprint(\"val_accuracy =\", np.max(study.best_trial.user_attrs[\"trial_data\"][\"val_accuracy\"]))\n",
    "rprint(\"channels_selected =\", study.best_trial.user_attrs[\"trial_data\"][\"channels_selected\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moabb_model_optimization_quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
