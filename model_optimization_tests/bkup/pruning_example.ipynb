{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.pick_types is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.pick_channels_regexp is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.channel_type is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "/home/arazzz/anaconda3/envs/moabb_model_optimization/lib/python3.11/site-packages/moabb/pipelines/__init__.py:26: ModuleNotFoundError: Tensorflow is not installed. You won't be able to use these MOABB pipelines if you attempt to do so.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import glob\n",
    "import pickle\n",
    "import tqdm\n",
    "import copy\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mne\n",
    "from rich import print as rprint\n",
    "from rich.pretty import pprint as rpprint\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "# JAX + Keras\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"0\"\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Permute, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.layers import SeparableConv2D, DepthwiseConv2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import SpatialDropout2D\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.layers import Input, Flatten\n",
    "from keras.constraints import max_norm\n",
    "from keras import backend as K\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, TimeDistributed, BatchNormalization\n",
    "import h5py\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Dataset\n",
    "from custom_datasets.fatigue_mi import FatigueMI\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 17:34:53.637499: W external/xla/xla/service/gpu/nvptx_compiler.cc:744] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.107). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "SKLRNG = 42\n",
    "RNG = jax.random.PRNGKey(SKLRNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding metadata with 3 columns\n"
     ]
    }
   ],
   "source": [
    "def data_generator(dataset, subjects = [1], channel_idx = [], filters = ([8, 32],), sfreq = 250):\n",
    "\n",
    "    find_events = lambda raw, event_id: mne.find_events(raw, shortest_event=0, verbose=False) if len(mne.utils._get_stim_channel(None, raw.info, raise_error=False)) > 0 else mne.events_from_annotations(raw, event_id=event_id, verbose=False)[0]\n",
    "    \n",
    "    data = dataset.get_data(subjects=subjects)\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    metadata = []\n",
    "\n",
    "    for subject_id in data.keys():\n",
    "        for session_id in data[subject_id].keys():\n",
    "            for run_id in data[subject_id][session_id].keys():\n",
    "                raw = data[subject_id][session_id][run_id]\n",
    "                \n",
    "                for fmin, fmax in filters:\n",
    "                    raw = raw.filter(l_freq = fmin, h_freq = fmax, method = 'iir', picks = 'eeg', verbose = False)\n",
    "                \n",
    "                events = find_events(raw, dataset.event_id)\n",
    "\n",
    "                tmin = dataset.interval[0]\n",
    "                tmax = dataset.interval[1]\n",
    "\n",
    "                channels = np.asarray(raw.info['ch_names'])[channel_idx] if len(channel_idx) > 0 else np.asarray(raw.info['ch_names'])\n",
    "\n",
    "                # rpprint(channels)\n",
    "                \n",
    "                stim_channels = mne.utils._get_stim_channel(None, raw.info, raise_error=False)\n",
    "                picks = mne.pick_channels(raw.info[\"ch_names\"], include=channels, exclude=stim_channels, ordered=True)\n",
    "\n",
    "                x = mne.Epochs(\n",
    "                    raw,\n",
    "                    events,\n",
    "                    event_id=dataset.event_id,\n",
    "                    tmin=tmin,\n",
    "                    tmax=tmax,\n",
    "                    proj=False,\n",
    "                    baseline=None,\n",
    "                    preload=True,\n",
    "                    verbose=False,\n",
    "                    picks=picks,\n",
    "                    event_repeated=\"drop\",\n",
    "                    on_missing=\"ignore\",\n",
    "                )\n",
    "                x_events = x.events\n",
    "                inv_events = {k: v for v, k in dataset.event_id.items()}\n",
    "                labels = [inv_events[e] for e in x_events[:, -1]]\n",
    "\n",
    "                # rpprint({\n",
    "                #     \"X\": np.asarray(x.get_data(copy=False)).shape,\n",
    "                #     \"y\": np.asarray(labels).shape,\n",
    "                #     \"channels selected\": np.asarray(raw.info['ch_names'])[channel_idx]\n",
    "                # })\n",
    "\n",
    "                # x.plot(scalings=\"auto\")\n",
    "                # display(x.info)\n",
    "                \n",
    "                x_resampled = x.resample(sfreq) # Resampler_Epoch\n",
    "                x_resampled_data = x_resampled.get_data(copy=False) # Convert_Epoch_Array\n",
    "                x_resampled_data_standard_scaler = np.asarray([\n",
    "                    StandardScaler().fit_transform(x_resampled_data[i])\n",
    "                    for i in np.arange(x_resampled_data.shape[0])\n",
    "                ]) # Standard_Scaler_Epoch\n",
    "\n",
    "                # x_resampled.plot(scalings=\"auto\")\n",
    "                # display(x_resampled.info)\n",
    "\n",
    "                n = x_resampled_data_standard_scaler.shape[0]\n",
    "                # n = x.get_data(copy=False).shape[0]\n",
    "                met = pd.DataFrame(index=range(n))\n",
    "                met[\"subject\"] = subject_id\n",
    "                met[\"session\"] = session_id\n",
    "                met[\"run\"] = run_id\n",
    "                x.metadata = met.copy()\n",
    "                \n",
    "                # X.append(x_resampled_data_standard_scaler)\n",
    "                X.append(x)\n",
    "                y.append(labels)\n",
    "                metadata.append(met)\n",
    "\n",
    "    return np.concatenate(X, axis=0), np.concatenate(y), pd.concat(metadata, ignore_index=True)\n",
    "\n",
    "fat_dataset = FatigueMI()\n",
    "X, y, _ = data_generator(fat_dataset, subjects=[1], channel_idx=[], sfreq=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'X'</span>: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">108</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'y'</span>: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">108</span>,<span style=\"font-weight: bold\">)}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'X'\u001b[0m: \u001b[1m(\u001b[0m\u001b[1;36m108\u001b[0m, \u001b[1;36m20\u001b[0m, \u001b[1;36m256\u001b[0m\u001b[1m)\u001b[0m, \u001b[32m'y'\u001b[0m: \u001b[1m(\u001b[0m\u001b[1;36m108\u001b[0m,\u001b[1m)\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'NUM_SAMPLES'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'NUM_CHANNELS'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'NUM_CLASSES'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'NUM_SAMPLES'\u001b[0m: \u001b[1;36m256\u001b[0m, \u001b[32m'NUM_CHANNELS'\u001b[0m: \u001b[1;36m20\u001b[0m, \u001b[32m'NUM_CLASSES'\u001b[0m: \u001b[1;36m2\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_encoded = LabelEncoder().fit_transform(y) # 0 = left, 1 = right, 2 = unlabelled (!NOTE: should be ignored for binary classification)\n",
    "# rpprint(y_encoded)\n",
    "rprint({\n",
    "    \"X\": X.shape,\n",
    "    \"y\": y_encoded.shape\n",
    "})\n",
    "\n",
    "NUM_SAMPLES = X.shape[-1]\n",
    "NUM_CHANNELS = X.shape[-2]\n",
    "NUM_CLASSES = len(np.unique(y_encoded))\n",
    "\n",
    "rpprint({\n",
    "    \"NUM_SAMPLES\": NUM_SAMPLES,\n",
    "    \"NUM_CHANNELS\": NUM_CHANNELS,\n",
    "    \"NUM_CLASSES\": NUM_CLASSES\n",
    "})\n",
    "\n",
    "# sns.histplot(y_encoded); # Plot the class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'X_train'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'86 (79.63%)'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'X_test'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'22 (20.37%)'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'X_train'\u001b[0m: \u001b[32m'86 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m79.63%\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'X_test'\u001b[0m: \u001b[32m'22 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m20.37%\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TRAIN_SIZE = 0.8\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X.reshape(-1, NUM_SAMPLES, NUM_CHANNELS), y_encoded, test_size=TEST_SIZE, random_state=SKLRNG, stratify=None, shuffle=False)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X.reshape(-1, NUM_SAMPLES, NUM_CHANNELS), y_encoded, test_size=TEST_SIZE, random_state=SKLRNG, stratify=y_encoded, shuffle=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=TEST_SIZE, random_state=SKLRNG, stratify=y_encoded, shuffle=True)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=TEST_SIZE, random_state=SKLRNG, stratify=None, shuffle=False)\n",
    "\n",
    "total = X_train.shape[0] + X_test.shape[0]\n",
    "rpprint({\n",
    "    \"X_train\": f\"{X_train.shape[0]} ({X_train.shape[0] / total * 100:.2f}%)\",\n",
    "    \"X_test\": f\"{X_test.shape[0]} ({X_test.shape[0] / total * 100:.2f}%)\",\n",
    "}, expand_all=True)\n",
    "\n",
    "# batch_size = 32\n",
    "# batches = batch_generator(X_train, batch_size, RNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region Helper funcs\n",
    "def shallow_conv_net_square_layer(x):\n",
    "    return jnp.square(x)\n",
    "\n",
    "def shallow_conv_net_log_layer(x):\n",
    "    return jnp.log(jnp.clip(x, 1e-7, 10000))\n",
    "\n",
    "CUSTOM_OBJECTS = {\n",
    "    \"shallow_conv_net_square_layer\": shallow_conv_net_square_layer, \n",
    "    \"shallow_conv_net_log_layer\": shallow_conv_net_log_layer \n",
    "}\n",
    "# endregion Helper funcs\n",
    "\n",
    "# region Models\n",
    "def shallow_conv_net(nb_classes = NUM_CLASSES, channels = NUM_CHANNELS, samples = NUM_SAMPLES, **kwargs):\n",
    "    \"\"\"\n",
    "        From: https://github.com/vlawhern/arl-eegmodels/blob/master/EEGModels.py\n",
    "    \"\"\"\n",
    "    \n",
    "    _POOL_SIZE_D2_ = kwargs.get(\"pool_size_d2\", 35)\n",
    "    _STRIDES_D2_ = kwargs.get(\"strides_d2\", 7)\n",
    "    _CONV_FILTERS_D2_ = kwargs.get(\"conv_filters_d2\", 13)\n",
    "\n",
    "    _POOL_SIZE_ = kwargs.get(\"pool_size\", (1, _POOL_SIZE_D2_))\n",
    "    _STRIDES_ = kwargs.get(\"strides\", (1, _STRIDES_D2_))\n",
    "    _CONV_FILTERS_ = kwargs.get(\"conv_filters\", (1, _CONV_FILTERS_D2_))\n",
    "\n",
    "    _CONV2D_1_UNITS_ = kwargs.get(\"conv2d_1_units\", 40)\n",
    "    _CONV2D_2_UNITS_ = kwargs.get(\"conv2d_2_units\", 40)\n",
    "    _L2_REG_1_ = kwargs.get(\"l2_reg_1\", 0.01)\n",
    "    _L2_REG_2_ = kwargs.get(\"l2_reg_2\", 0.01)\n",
    "    _L2_REG_3_ = kwargs.get(\"l2_reg_3\", 0.01)\n",
    "    _DROPOUT_RATE_ = kwargs.get(\"dropout_rate\", 0.5)\n",
    "\n",
    "    input_main   = Input(shape=(channels, samples, 1))\n",
    "    block1       = Conv2D(_CONV2D_1_UNITS_, _CONV_FILTERS_,\n",
    "                                 input_shape=(channels, samples, 1),\n",
    "                                 kernel_constraint = max_norm(2., axis=(0,1,2)), kernel_regularizer=keras.regularizers.L2(_L2_REG_1_))(input_main)\n",
    "    # block1       = Conv2D(40, (channels, 1), use_bias=False, \n",
    "    #                       kernel_constraint = max_norm(2., axis=(0,1,2)))(block1)\n",
    "    block1       = Conv2D(_CONV2D_2_UNITS_, (channels, 1), use_bias=False, \n",
    "                          kernel_constraint = max_norm(2., axis=(0,1,2)), kernel_regularizer=keras.regularizers.L2(_L2_REG_2_))(block1)\n",
    "    block1       = BatchNormalization(epsilon=1e-05, momentum=0.9)(block1)\n",
    "    block1       = Activation(shallow_conv_net_square_layer)(block1)\n",
    "    block1       = AveragePooling2D(pool_size=_POOL_SIZE_, strides=_STRIDES_)(block1)\n",
    "    block1       = Activation(shallow_conv_net_log_layer)(block1)\n",
    "    block1       = Dropout(_DROPOUT_RATE_)(block1)\n",
    "    flatten      = Flatten()(block1)\n",
    "    # dense        = Dense(nb_classes, kernel_constraint = max_norm(0.5))(flatten)\n",
    "    dense        = Dense(nb_classes, kernel_constraint = max_norm(0.5), kernel_regularizer=keras.regularizers.L2(_L2_REG_3_))(flatten)\n",
    "    softmax      = Activation('softmax')(dense)\n",
    "    \n",
    "    return Model(inputs=input_main, outputs=softmax)\n",
    "\n",
    "def lstm_cnn_net(nb_classes = NUM_CLASSES, channels = NUM_CHANNELS, samples = NUM_SAMPLES, **kwargs):\n",
    "\n",
    "    _CONV1D_1_UNITS_ = kwargs.get(\"conv1d_1_units\", 40)\n",
    "    _CONV1D_1_KERNEL_SIZE_ = kwargs.get(\"conv1d_1_kernel_size\", 20)\n",
    "    _CONV1D_1_STRIDES_ = kwargs.get(\"conv1d_1_strides\", 4)\n",
    "    _CONV1D_1_MAXPOOL_SIZE_ = kwargs.get(\"conv1d_1_maxpool_size\", 4)\n",
    "    _CONV1D_1_MAXPOOL_STRIDES_ = kwargs.get(\"conv1d_1_maxpool_strides\", 4)\n",
    "    _LSTM_1_UNITS_ = kwargs.get(\"lstm_1_units\", 50)\n",
    "    _L2_REG_1_ = kwargs.get(\"l2_reg_1\", 0.01)\n",
    "    _L2_REG_2_ = kwargs.get(\"l2_reg_2\", 0.01)\n",
    "    _L2_REG_3_ = kwargs.get(\"l2_reg_3\", 0.01)\n",
    "    _DROPOUT_RATE_1_ = kwargs.get(\"dropout_rate_1\", 0.5)\n",
    "    _DROPOUT_RATE_2_ = kwargs.get(\"dropout_rate_2\", 0.5)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(samples, channels)))\n",
    "\n",
    "    # add 1-layer cnn\n",
    "    model.add(Conv1D(_CONV1D_1_UNITS_, kernel_size=_CONV1D_1_KERNEL_SIZE_, strides=_CONV1D_1_STRIDES_, kernel_regularizer=keras.regularizers.L2(_L2_REG_1_)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(_DROPOUT_RATE_1_))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=_CONV1D_1_MAXPOOL_SIZE_, strides=_CONV1D_1_MAXPOOL_STRIDES_))\n",
    "\n",
    "\n",
    "    # add 1-layer lstm\n",
    "    model.add(LSTM(_LSTM_1_UNITS_, return_sequences=True, stateful=False, kernel_regularizer=keras.regularizers.L2(_L2_REG_2_)))\n",
    "    model.add(Dropout(_DROPOUT_RATE_2_))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(nb_classes, activation='softmax', kernel_regularizer=keras.regularizers.L2(_L2_REG_3_)))\n",
    "    \n",
    "    return model\n",
    "# endregion Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'sfreq'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">300</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">160</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'pool_size_d2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'strides_d2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'conv_filters_d2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'conv2d_1_units'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">180</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'conv2d_2_units'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'l2_reg_1'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7516306360258819</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'l2_reg_2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8663258330477787</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'l2_reg_3'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6564614823286843</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'dropout_rate'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'sfreq'\u001b[0m: \u001b[1;36m300\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'batch_size'\u001b[0m: \u001b[1;36m160\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'pool_size_d2'\u001b[0m: \u001b[1;36m15\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'strides_d2'\u001b[0m: \u001b[1;36m17\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'conv_filters_d2'\u001b[0m: \u001b[1;36m40\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'conv2d_1_units'\u001b[0m: \u001b[1;36m180\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'conv2d_2_units'\u001b[0m: \u001b[1;36m30\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'l2_reg_1'\u001b[0m: \u001b[1;36m0.7516306360258819\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'l2_reg_2'\u001b[0m: \u001b[1;36m0.8663258330477787\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'l2_reg_3'\u001b[0m: \u001b[1;36m0.6564614823286843\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'dropout_rate'\u001b[0m: \u001b[1;36m0.4\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">test_accuracy = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8636363744735718</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "test_accuracy = \u001b[1;36m0.8636363744735718\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">val_accuracy = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8333333134651184</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "val_accuracy = \u001b[1;36m0.8333333134651184\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">channels_selected = <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Fz'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'C4'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'Fp2'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'T5'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'O2'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'F7'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'F8'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'A2'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'T6'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'T4'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "channels_selected = \u001b[1m[\u001b[0m\u001b[32m'Fz'\u001b[0m \u001b[32m'C4'\u001b[0m \u001b[32m'Fp2'\u001b[0m \u001b[32m'T5'\u001b[0m \u001b[32m'O2'\u001b[0m \u001b[32m'F7'\u001b[0m \u001b[32m'F8'\u001b[0m \u001b[32m'A2'\u001b[0m \u001b[32m'T6'\u001b[0m \u001b[32m'T4'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load subject model\n",
    "# subject_data = np.load(\"best_subjects_search_results/subject_12_shallow_conv_net_3a2d5dc0-13c5-4bdb-86dc-b9a4afb56ce2.npy\", allow_pickle=True).item()\n",
    "subject_data = np.load(\"../temp/[1]/292cbc92b8cf46da9986fe7d8447819f/model/study_best_trial.npy\", allow_pickle=True).item()\n",
    "subject_trial_data = subject_data.user_attrs['trial_data']\n",
    "subject_model_json = subject_trial_data['model']\n",
    "subject_model = keras.models.model_from_json(subject_model_json, custom_objects=CUSTOM_OBJECTS)\n",
    "subject_model_weights = subject_trial_data['weights']\n",
    "subject_model.set_weights(subject_model_weights)\n",
    "\n",
    "def channels_to_channels_idx(channels, dataset):\n",
    "    all_channels = dataset.get_data(subjects=[1])[1]['0']['0'].info['ch_names'][:-1]\n",
    "    channels_dict = { k: (True if k in channels else False) for k in all_channels }\n",
    "    channels_idx = [i for i, v in enumerate(channels_dict.values()) if v]\n",
    "    return channels_idx\n",
    "\n",
    "subject_channels_idx = channels_to_channels_idx(subject_trial_data['channels_selected'], fat_dataset)\n",
    "subject_sfreq = subject_data.params['sfreq'] if 'sfreq' in subject_data.params else 128\n",
    "subject_data_hyper_params = { k: v for k, v in subject_data.params.items() if not k.startswith('channels_') }\n",
    "subject_batch_size = subject_data.params['batch_size']\n",
    "\n",
    "rpprint({ k: v for k, v in subject_data.params.items() if not k.startswith(\"channels\") })\n",
    "rprint(\"test_accuracy =\", subject_data.user_attrs[\"trial_data\"][\"test_accuracy\"])\n",
    "rprint(\"val_accuracy =\", np.max(subject_data.user_attrs[\"trial_data\"][\"val_accuracy\"]))\n",
    "rprint(\"channels_selected =\", subject_data.user_attrs[\"trial_data\"][\"channels_selected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling frequency of the instance is already 300.0, returning unmodified.\n",
      "Adding metadata with 3 columns\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.9074 - loss: 50.4109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[50.41094970703125, 0.9074074029922485]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y, _ = data_generator(fat_dataset, subjects=[1], channel_idx=subject_channels_idx, sfreq=subject_sfreq)\n",
    "# X, y = X[0:50].reshape(len(X[0:50]), len(subject_channels_idx), -1), y[0:50]\n",
    "y_encoded = LabelEncoder().fit_transform(y)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=SKLRNG, shuffle=True, stratify=y_encoded)\n",
    "\n",
    "# subject_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "subject_model.evaluate(X, y_encoded, batch_size=subject_batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moabb_model_optimization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
