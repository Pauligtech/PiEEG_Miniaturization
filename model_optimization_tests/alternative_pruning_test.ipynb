{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
     ]    
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use the get_shape_from_baseconcar, InputShapeSetterEEG, BraindecodeDatasetLoaderyou need to install `braindecode`.`pip install braindecode` or Please refer to `https://braindecode.org`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
   "source": [
    "# region General Imports\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import glob\n",
    "import pickle\n",
    "import tqdm\n",
    "import copy\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mne\n",
    "from rich import print as rprint\n",
    "from rich.pretty import pprint as rpprint\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "# endregion General Imports\n",
    "\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "import tensorflow_model_optimization as tfmot\n",
    "%load_ext tensorboard\n",
    "    \n",
    "# os.environ[\"KERAS_BACKEND\"] = \"tf\"\n",
    "# os.environ[\"TF_USE_LEGACY_KERAS\"] = \"0\"\n",
    "# import jax\n",
    "# import jax.numpy as jnp\n",
    "# import keras\n",
    "    \n",
    "# region Keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Permute, Dropout\n",
    "from keras.layers import (\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    AveragePooling2D,\n",
    "    Conv1D,\n",
    "    MaxPooling1D,\n",
    "    AveragePooling1D,\n",
    ")\n",
    "from keras.layers import SeparableConv2D, DepthwiseConv2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import SpatialDropout2D\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.layers import Input, Flatten\n",
    "from keras.constraints import max_norm\n",
    "from keras import backend as K\n",
    "# endregion Keras\n",
    "\n",
    "from custom_datasets.fatigue_mi import FatigueMI\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, TimeDistributed, BatchNormalization\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from model_optim.utils import channels_to_channels_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKLRNG = 42\n",
    "# RNG = jax.random.PRNGKey(SKLRNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region Helper funcs\n",
    "def shallow_conv_net_square_layer(x):\n",
    "    return tf.math.square(x)\n",
    "\n",
    "def shallow_conv_net_log_layer(x):\n",
    "    return tf.math.log(tf.clip_by_value(x, 1e-7, 10000))\n",
    "\n",
    "CUSTOM_OBJECTS = {\n",
    "    \"shallow_conv_net_square_layer\": shallow_conv_net_square_layer, \n",
    "    \"shallow_conv_net_log_layer\": shallow_conv_net_log_layer \n",
    "}\n",
    "# endregion Helper funcs\n",
    "\n",
    "# region Models\n",
    "def shallow_conv_net(\n",
    "    nb_classes, channels, samples, **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    From: https://github.com/vlawhern/arl-eegmodels/blob/master/EEGModels.py\n",
    "    \"\"\"\n",
    "\n",
    "    _POOL_SIZE_D2_ = kwargs.get(\"pool_size_d2\", 35)\n",
    "    _STRIDES_D2_ = kwargs.get(\"strides_d2\", 7)\n",
    "    _CONV_FILTERS_D2_ = kwargs.get(\"conv_filters_d2\", 13)\n",
    "\n",
    "    _POOL_SIZE_ = kwargs.get(\"pool_size\", (1, _POOL_SIZE_D2_))\n",
    "    _STRIDES_ = kwargs.get(\"strides\", (1, _STRIDES_D2_))\n",
    "    _CONV_FILTERS_ = kwargs.get(\"conv_filters\", (1, _CONV_FILTERS_D2_))\n",
    "\n",
    "    _CONV2D_1_UNITS_ = kwargs.get(\"conv2d_1_units\", 40)\n",
    "    _CONV2D_2_UNITS_ = kwargs.get(\"conv2d_2_units\", 40)\n",
    "    _L2_REG_1_ = kwargs.get(\"l2_reg_1\", 0.01)\n",
    "    _L2_REG_2_ = kwargs.get(\"l2_reg_2\", 0.01)\n",
    "    _L2_REG_3_ = kwargs.get(\"l2_reg_3\", 0.01)\n",
    "    _DROPOUT_RATE_ = kwargs.get(\"dropout_rate\", 0.5)\n",
    "\n",
    "    input_main = Input(shape=(channels, samples, 1))\n",
    "    block1 = Conv2D(\n",
    "        _CONV2D_1_UNITS_,\n",
    "        _CONV_FILTERS_,\n",
    "        input_shape=(channels, samples, 1),\n",
    "        kernel_constraint=max_norm(2.0, axis=(0, 1, 2)),\n",
    "        kernel_regularizer=keras.regularizers.L2(_L2_REG_1_),\n",
    "    )(input_main)\n",
    "    # block1       = Conv2D(40, (channels, 1), use_bias=False,\n",
    "    #                       kernel_constraint = max_norm(2., axis=(0,1,2)))(block1)\n",
    "    block1 = Conv2D(\n",
    "        _CONV2D_2_UNITS_,\n",
    "        (channels, 1),\n",
    "        use_bias=False,\n",
    "        kernel_constraint=max_norm(2.0, axis=(0, 1, 2)),\n",
    "        kernel_regularizer=keras.regularizers.L2(_L2_REG_2_),\n",
    "    )(block1)\n",
    "    block1 = BatchNormalization(epsilon=1e-05, momentum=0.9)(block1)\n",
    "    block1 = Activation(shallow_conv_net_square_layer)(block1)\n",
    "    block1 = AveragePooling2D(pool_size=_POOL_SIZE_, strides=_STRIDES_)(block1)\n",
    "    block1 = Activation(shallow_conv_net_log_layer)(block1)\n",
    "    block1 = Dropout(_DROPOUT_RATE_)(block1)\n",
    "    flatten = Flatten()(block1)\n",
    "    # dense        = Dense(nb_classes, kernel_constraint = max_norm(0.5))(flatten)\n",
    "    dense = Dense(\n",
    "        nb_classes,\n",
    "        kernel_constraint=max_norm(0.5),\n",
    "        kernel_regularizer=keras.regularizers.L2(_L2_REG_3_),\n",
    "    )(flatten)\n",
    "    softmax = Activation(\"softmax\")(dense)\n",
    "\n",
    "    return Model(inputs=input_main, outputs=softmax)\n",
    "\n",
    "# endregion Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_best_trials = glob.glob('./temp_v2/**/model/study_best_trial.npy', recursive=True)\n",
    "subject_best_trials = sorted(subject_best_trials, key=lambda x: os.path.getmtime(x))\n",
    "subject_best_trials = sorted(subject_best_trials, key=lambda x: int(re.compile(r\"\\[.*\\]\").search(x).group(0).strip(\"[]\")))\n",
    "\n",
    "# subject_best_trials = glob.glob('./temp/**/model/study_best_trial.npy', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(dataset, subjects = [1], channel_idx = [], filters = ([8, 32],), sfreq = 250):\n",
    "\n",
    "    find_events = lambda raw, event_id: mne.find_events(raw, shortest_event=0, verbose=False) if len(mne.utils._get_stim_channel(None, raw.info, raise_error=False)) > 0 else mne.events_from_annotations(raw, event_id=event_id, verbose=False)[0]\n",
    "    \n",
    "    data = dataset.get_data(subjects=subjects)\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    metadata = []\n",
    "\n",
    "    for subject_id in data.keys():\n",
    "        for session_id in data[subject_id].keys():\n",
    "            for run_id in data[subject_id][session_id].keys():\n",
    "                raw = data[subject_id][session_id][run_id]\n",
    "                \n",
    "                for fmin, fmax in filters:\n",
    "                    raw = raw.filter(l_freq = fmin, h_freq = fmax, method = 'iir', picks = 'eeg', verbose = False)\n",
    "                \n",
    "                events = find_events(raw, dataset.event_id)\n",
    "\n",
    "                tmin = dataset.interval[0]\n",
    "                tmax = dataset.interval[1]\n",
    "\n",
    "                channels = np.asarray(raw.info['ch_names'])[channel_idx] if len(channel_idx) > 0 else np.asarray(raw.info['ch_names'])\n",
    "\n",
    "                # rpprint(channels)\n",
    "                \n",
    "                stim_channels = mne.utils._get_stim_channel(None, raw.info, raise_error=False)\n",
    "                picks = mne.pick_channels(raw.info[\"ch_names\"], include=channels, exclude=stim_channels, ordered=True)\n",
    "\n",
    "                x = mne.Epochs(\n",
    "                    raw,\n",
    "                    events,\n",
    "                    event_id=dataset.event_id,\n",
    "                    tmin=tmin,\n",
    "                    tmax=tmax,\n",
    "                    proj=False,\n",
    "                    baseline=None,\n",
    "                    preload=True,\n",
    "                    verbose=False,\n",
    "                    picks=picks,\n",
    "                    event_repeated=\"drop\",\n",
    "                    on_missing=\"ignore\",\n",
    "                )\n",
    "                x_events = x.events\n",
    "                inv_events = {k: v for v, k in dataset.event_id.items()}\n",
    "                labels = [inv_events[e] for e in x_events[:, -1]]\n",
    "\n",
    "                # rpprint({\n",
    "                #     \"X\": np.asarray(x.get_data(copy=False)).shape,\n",
    "                #     \"y\": np.asarray(labels).shape,\n",
    "                #     \"channels selected\": np.asarray(raw.info['ch_names'])[channel_idx]\n",
    "                # })\n",
    "\n",
    "                # x.plot(scalings=\"auto\")\n",
    "                # display(x.info)\n",
    "                \n",
    "                x_resampled = x.resample(sfreq) # Resampler_Epoch\n",
    "                x_resampled_data = x_resampled.get_data(copy=False) # Convert_Epoch_Array\n",
    "                x_resampled_data_standard_scaler = np.asarray([\n",
    "                    StandardScaler().fit_transform(x_resampled_data[i])\n",
    "                    for i in np.arange(x_resampled_data.shape[0])\n",
    "                ]) # Standard_Scaler_Epoch\n",
    "\n",
    "                # x_resampled.plot(scalings=\"auto\")\n",
    "                # display(x_resampled.info)\n",
    "\n",
    "                n = x_resampled_data_standard_scaler.shape[0]\n",
    "                # n = x.get_data(copy=False).shape[0]\n",
    "                met = pd.DataFrame(index=range(n))\n",
    "                met[\"subject\"] = subject_id\n",
    "                met[\"session\"] = session_id\n",
    "                met[\"run\"] = run_id\n",
    "                x.metadata = met.copy()\n",
    "                \n",
    "                # X.append(x_resampled_data_standard_scaler)\n",
    "                X.append(x)\n",
    "                y.append(labels)\n",
    "                metadata.append(met)\n",
    "\n",
    "    return np.concatenate(X, axis=0), np.concatenate(y), pd.concat(metadata, ignore_index=True)\n",
    "\n",
    "fat_dataset = FatigueMI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'./temp_v2/[1]/292cbc92b8cf46da9986fe7d8447819f/model/study_best_trial.npy'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'./temp_v2/[2]/6405d11e654b42aca9df48458c67ecde/model/study_best_trial.npy'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'./temp_v2/[3]/9c1b753483db409a90eab7b7149b8af8/model/study_best_trial.npy'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'./temp_v2/[4]/3623cb4ba1ad4a908c9098f5297a6778/model/study_best_trial.npy'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'./temp_v2/[5]/d199c9c2ac924b238693f158eb88f675/model/study_best_trial.npy'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'./temp_v2/[6]/39d01251ff494106bf04f8a2cffcdd74/model/study_best_trial.npy'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'./temp_v2/[7]/aabe056cd1954a6f92ab47d84c86b1b8/model/study_best_trial.npy'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'./temp_v2/[8]/9fd82ec44ef3496da6307b57ecf4532f/model/study_best_trial.npy'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'./temp_v2/[9]/63a8c87ffc02471893db5ac9a0781946/model/study_best_trial.npy'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'./temp_v2/[10]/c2cc69dca74d4bfa81722cd634e6403e/model/study_best_trial.npy'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'./temp_v2/[11]/e0643f9a780146a4adc15ddd4a9ff053/model/study_best_trial.npy'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'./temp_v2/[12]/96dc576945fb4f2db582d66ae1d2c8ce/model/study_best_trial.npy'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'./temp_v2/[13]/1b189965ada44ff99e73fa145cd3901d/model/study_best_trial.npy'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'./temp_v2/[14]/a3304348c7094d02a024828ede942cda/model/study_best_trial.npy'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'./temp_v2/\u001b[0m\u001b[32m[\u001b[0m\u001b[32m1\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/292cbc92b8cf46da9986fe7d8447819f/model/study_best_trial.npy'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'./temp_v2/\u001b[0m\u001b[32m[\u001b[0m\u001b[32m2\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/6405d11e654b42aca9df48458c67ecde/model/study_best_trial.npy'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'./temp_v2/\u001b[0m\u001b[32m[\u001b[0m\u001b[32m3\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/9c1b753483db409a90eab7b7149b8af8/model/study_best_trial.npy'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'./temp_v2/\u001b[0m\u001b[32m[\u001b[0m\u001b[32m4\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/3623cb4ba1ad4a908c9098f5297a6778/model/study_best_trial.npy'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'./temp_v2/\u001b[0m\u001b[32m[\u001b[0m\u001b[32m5\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/d199c9c2ac924b238693f158eb88f675/model/study_best_trial.npy'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'./temp_v2/\u001b[0m\u001b[32m[\u001b[0m\u001b[32m6\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/39d01251ff494106bf04f8a2cffcdd74/model/study_best_trial.npy'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'./temp_v2/\u001b[0m\u001b[32m[\u001b[0m\u001b[32m7\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/aabe056cd1954a6f92ab47d84c86b1b8/model/study_best_trial.npy'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'./temp_v2/\u001b[0m\u001b[32m[\u001b[0m\u001b[32m8\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/9fd82ec44ef3496da6307b57ecf4532f/model/study_best_trial.npy'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'./temp_v2/\u001b[0m\u001b[32m[\u001b[0m\u001b[32m9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/63a8c87ffc02471893db5ac9a0781946/model/study_best_trial.npy'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'./temp_v2/\u001b[0m\u001b[32m[\u001b[0m\u001b[32m10\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/c2cc69dca74d4bfa81722cd634e6403e/model/study_best_trial.npy'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'./temp_v2/\u001b[0m\u001b[32m[\u001b[0m\u001b[32m11\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/e0643f9a780146a4adc15ddd4a9ff053/model/study_best_trial.npy'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'./temp_v2/\u001b[0m\u001b[32m[\u001b[0m\u001b[32m12\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/96dc576945fb4f2db582d66ae1d2c8ce/model/study_best_trial.npy'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'./temp_v2/\u001b[0m\u001b[32m[\u001b[0m\u001b[32m13\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/1b189965ada44ff99e73fa145cd3901d/model/study_best_trial.npy'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'./temp_v2/\u001b[0m\u001b[32m[\u001b[0m\u001b[32m14\u001b[0m\u001b[32m]\u001b[0m\u001b[32m/a3304348c7094d02a024828ede942cda/model/study_best_trial.npy'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rpprint(subject_best_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_prune_dense_layer(k_weights, b_weights, k_sparsity):\n",
    "    \"\"\"\n",
    "    Takes in matrices of kernel and bias weights (for a dense\n",
    "      layer) and returns the unit-pruned versions of each\n",
    "    Args:\n",
    "      k_weights: 2D matrix of the \n",
    "      b_weights: 1D matrix of the biases of a dense layer\n",
    "      k_sparsity: percentage of weights to set to 0\n",
    "    Returns:\n",
    "      kernel_weights: sparse matrix with same shape as the original\n",
    "        kernel weight matrix\n",
    "      bias_weights: sparse array with same shape as the original\n",
    "        bias array\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy the kernel weights and get ranked indeces of the\n",
    "    # column-wise L2 Norms\n",
    "    kernel_weights = np.copy(k_weights)\n",
    "    ind = np.argsort(np.linalg.norm(kernel_weights, axis=0))\n",
    "        \n",
    "    # Number of indexes to set to 0\n",
    "    cutoff = int(len(ind)*k_sparsity)\n",
    "    # The indexes in the 2D kernel weight matrix to set to 0\n",
    "    sparse_cutoff_inds = ind[0:cutoff]\n",
    "    kernel_weights[:,sparse_cutoff_inds] = 0.\n",
    "        \n",
    "    # Copy the bias weights and get ranked indeces of the abs\n",
    "    bias_weights = np.copy(b_weights)\n",
    "    # The indexes in the 1D bias weight matrix to set to 0\n",
    "    # Equal to the indexes of the columns that were removed in this case\n",
    "    #sparse_cutoff_inds\n",
    "    bias_weights[sparse_cutoff_inds] = 0.\n",
    "    \n",
    "    return kernel_weights, bias_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_trial(subject_best_trial):\n",
    "    model = np.load(subject_best_trials[0], allow_pickle=True).item()\n",
    "    model_info = {\n",
    "        \"subject\": model.user_attrs[\"trial_data\"][\"subject\"] if hasattr(model.user_attrs[\"trial_data\"], \"subject\") else int(re.compile(r\"\\[.*\\]\").search(model.user_attrs[\"trial_data\"][\"data_path\"]).group(0).strip(\"[]\")),\n",
    "        \"sfreq\": model.params[\"sfreq\"] if \"sfreq\" in model.params else 128,\n",
    "        \"batch_size\": model.params[\"batch_size\"] if \"batch_size\" in model.params else 128,\n",
    "        \"channels_selected\": model.user_attrs[\"trial_data\"][\"channels_selected\"],\n",
    "        \"channels_idx_selected\": channels_to_channels_idx(model.user_attrs[\"trial_data\"][\"channels_selected\"], fat_dataset.get_data(subjects=[1])[1]['0']['0'].info['ch_names'][:-1]),\n",
    "        \"model\": tf.keras.models.model_from_json(model.user_attrs[\"trial_data\"][\"model\"], custom_objects=CUSTOM_OBJECTS),\n",
    "        \"test_acc\": model.user_attrs[\"trial_data\"][\"test_accuracy\"],\n",
    "        \"model_name\": model.user_attrs[\"trial_data\"][\"model_name\"] if hasattr(model.user_attrs[\"trial_data\"], \"model_name\") else \"shallow_conv_net\"\n",
    "    }\n",
    "    if \"weights\" in model.user_attrs[\"trial_data\"]:\n",
    "        model_info[\"model\"].set_weights(model.user_attrs[\"trial_data\"][\"weights\"])\n",
    "    elif \"model_weights\" in model.user_attrs[\"trial_data\"]:\n",
    "        model_info[\"model\"].set_weights(model.user_attrs[\"trial_data\"][\"model_weights\"])\n",
    "    \n",
    "    return model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "\n",
       "Loading best trial model<span style=\"color: #808000; text-decoration-color: #808000\">...</span> .<span style=\"color: #800080; text-decoration-color: #800080\">/temp_v2/</span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span><span style=\"color: #800080; text-decoration-color: #800080\">/292cbc92b8cf46da9986fe7d8447819f/model/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">study_best_trial.npy</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n",
       "\n",
       "Loading best trial model\u001b[33m...\u001b[0m .\u001b[35m/temp_v2/\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[35m/292cbc92b8cf46da9986fe7d8447819f/model/\u001b[0m\u001b[95mstudy_best_trial.npy\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling frequency of the instance is already 300.0, returning unmodified.\n",
      "Adding metadata with 3 columns\n"
     ]
    }
   ],
   "source": [
    "for subject_best_trial in subject_best_trials[0:1]:\n",
    "\n",
    "    rprint(\"\\n\\n\\nLoading best trial model...\", subject_best_trial)\n",
    "\n",
    "    model_info = load_best_trial(subject_best_trial)\n",
    "    subject = model_info[\"subject\"]\n",
    "    model_name = model_info[\"model_name\"]\n",
    "\n",
    "    X, y, _ = data_generator(fat_dataset, subjects=[model_info[\"subject\"]], channel_idx=model_info[\"channels_idx_selected\"], sfreq=model_info[\"sfreq\"])\n",
    "    y_encoded = LabelEncoder().fit_transform(y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=SKLRNG, shuffle=True, stratify=y_encoded)\n",
    "    train_test_data = { \"X_train\": X_train, \"X_test\": X_test, \"y_train\": y_train, \"y_test\": y_test }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k% weight sparsity:  0.4 \tTest loss: 24.94406 \tTest accuracy: 68.18 %%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf_keras.src.engine.functional.Functional at 0x7f60d8b26d90>,\n",
       " [24.944063186645508, 0.6818181872367859])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weight_prune_dense_layer(k_weights, b_weights, k_sparsity):\n",
    "    # Copy the kernel weights and get ranked indeces of the abs\n",
    "    kernel_weights = np.copy(k_weights)\n",
    "    kernel_weight_idx_by_magnitude = np.argsort(np.abs(kernel_weights), axis=None) # rank the individual weights in weight matrix according to their magnitude (absolute value)\n",
    "\n",
    "    kernel_weights_sparse_idx = kernel_weight_idx_by_magnitude[0:int(len(kernel_weight_idx_by_magnitude)*k_sparsity)]\n",
    "    kernel_weights[np.unravel_index(kernel_weights_sparse_idx, kernel_weights.shape) if len(kernel_weights_sparse_idx) > 0 else kernel_weights_sparse_idx] = 0\n",
    "\n",
    "    if b_weights is None:\n",
    "        return kernel_weights, None\n",
    "    \n",
    "    bias_weights = np.copy(b_weights)\n",
    "    bias_weights_idx_by_magnitude = np.argsort(np.abs(bias_weights), axis=None) # rank the individual weights in weight matrix according to their magnitude (absolute value)\n",
    "\n",
    "    bias_weights_sparse_idx = bias_weights_idx_by_magnitude[0:int(len(bias_weights_idx_by_magnitude)*k_sparsity)]\n",
    "    bias_weights[np.unravel_index(bias_weights_sparse_idx, bias_weights.shape)] = 0\n",
    "\n",
    "    return kernel_weights, bias_weights\n",
    "\n",
    "\n",
    "def sparsify_model(model, x_test, y_test, k_sparsity, pruning='weight', ignore_batch_norm = True):\n",
    "    keras.utils.get_custom_objects().update(CUSTOM_OBJECTS)\n",
    "    sparse_model = tf.keras.models.clone_model(model)\n",
    "    sparse_model.set_weights(model.get_weights())\n",
    "\n",
    "    assert ignore_batch_norm == True, print(\"Batch Normalization is not supported yet\")\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if \"input\" in layer.name or len(layer.trainable_weights) == 0:\n",
    "            continue\n",
    "        if \"batch_normalization\" in layer.name and ignore_batch_norm:\n",
    "            continue\n",
    "        W = layer.get_weights()[0]\n",
    "        b = None\n",
    "        if \"batch_normalization\" not in layer.name and layer.use_bias:\n",
    "            b = layer.get_weights()[1]\n",
    "\n",
    "        if pruning=='weight':\n",
    "            # rprint(layer.name, W.shape)\n",
    "            kernel_weights, bias_weights = weight_prune_dense_layer(W, b, k_sparsity)\n",
    "        # elif pruning=='unit':\n",
    "        #     kernel_weights, bias_weights = unit_prune_dense_layer(W, b, k_sparsity)\n",
    "\n",
    "        sparse_model.get_layer(layer.name).set_weights([kernel_weights, bias_weights] if b is not None else [kernel_weights])\n",
    "\n",
    "    sparse_model.compile(\n",
    "        loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    # Printing the the associated loss & Accuracy for the k% sparsity\n",
    "    score = sparse_model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('k% weight sparsity: ', k_sparsity,\n",
    "          '\\tTest loss: {:07.5f}'.format(score[0]),\n",
    "          '\\tTest accuracy: {:05.2f} %%'.format(score[1]*100.))\n",
    "    \n",
    "    return sparse_model, score\n",
    "\n",
    "\n",
    "sparsify_model(\n",
    "    model = model_info[\"model\"],\n",
    "    x_test = train_test_data[\"X_test\"],\n",
    "    y_test = train_test_data[\"y_test\"],\n",
    "    k_sparsity = 0.40,\n",
    "    pruning = 'weight'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'subject'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'sfreq'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">300</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">160</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'channels_selected'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">array</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008000; text-decoration-color: #008000\">'Fz'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'C4'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Fp2'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'T5'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'O2'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'F7'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'F8'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'A2'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'T6'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'T4'</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'&lt;U3'</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'channels_idx_selected'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'model'</span><span style=\"color: #000000; text-decoration-color: #000000\">: &lt;tf_keras.src.engine.functional.Functional object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7f60d8aafbd0</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'test_acc'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8636363744735718</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'shallow_conv_net'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'subject'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'sfreq'\u001b[0m: \u001b[1;36m300\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'batch_size'\u001b[0m: \u001b[1;36m160\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'channels_selected'\u001b[0m: \u001b[1;35marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[32m'Fz'\u001b[0m, \u001b[32m'C4'\u001b[0m, \u001b[32m'Fp2'\u001b[0m, \u001b[32m'T5'\u001b[0m, \u001b[32m'O2'\u001b[0m, \u001b[32m'F7'\u001b[0m, \u001b[32m'F8'\u001b[0m, \u001b[32m'A2'\u001b[0m, \u001b[32m'T6'\u001b[0m, \u001b[32m'T4'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│     \u001b[0m\u001b[33mdtype\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32mU3\u001b[0m\u001b[32m'\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'channels_idx_selected'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m5\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m10\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m12\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m14\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m15\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m16\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m17\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m18\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m19\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'model'\u001b[0m\u001b[39m: <tf_keras.src.engine.functional.Functional object at \u001b[0m\u001b[1;36m0x7f60d8aafbd0\u001b[0m\u001b[1m>\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'test_acc'\u001b[0m: \u001b[1;36m0.8636363744735718\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'model_name'\u001b[0m: \u001b[32m'shallow_conv_net'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rpprint(model_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 10, 601, 1)]      0         \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 10, 562, 180)      7380      \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 1, 562, 30)        54000     \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 1, 562, 30)        120       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_27 (Activation)  (None, 1, 562, 30)        0         \n",
      "                                                                 \n",
      " average_pooling2d_9 (Avera  (None, 1, 33, 30)         0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " activation_28 (Activation)  (None, 1, 33, 30)         0         \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 1, 33, 30)         0         \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 990)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 2)                 1982      \n",
      "                                                                 \n",
      " activation_29 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 63482 (247.98 KB)\n",
      "Trainable params: 63422 (247.74 KB)\n",
      "Non-trainable params: 60 (240.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_info[\"model\"].summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moabb_model_optimization_quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
